# Default Configuration for AI Research System
# This file contains sensible defaults for all configurable parameters.
# Copy to config.yaml and customize as needed.

# ============================================================================
# LLM Configuration
# ============================================================================
# LiteLLM supports 100+ providers - just change the model name!
# Examples:
#   OpenAI GPT-5: "gpt-5", "gpt-5-mini", "gpt-5-nano" (recommended: gpt-5-nano for cost)
#   OpenAI GPT-4: "gpt-4o", "gpt-4o-mini"
#   Anthropic: "claude-3-5-sonnet-20241022", "claude-3-opus-20240229"
#   Google: "gemini/gemini-2.0-flash-exp", "gemini/gemini-1.5-pro"
#   Local: "ollama/llama3", "ollama/mistral"
#
# Model Recommendations by Cost/Performance:
#   - gpt-5-nano: Lowest cost, good for simple queries (~10x cheaper than gpt-5-mini)
#   - gpt-5-mini: Best balance of cost/performance (current default)
#   - gpt-5: Highest quality, highest cost

llm:
  # Default model for all operations (can be overridden per-operation)
  default_model: "gemini/gemini-2.5-flash"

  # Operation-specific model configurations
  # Each can have: model, temperature, max_tokens

  query_generation:
    model: "gemini/gemini-2.5-flash"  # Model for generating database queries
    temperature: 0.7              # Balanced creativity/precision
    max_tokens: 500               # Queries are typically short

  refinement:
    model: "gemini/gemini-2.5-flash"  # Model for refining poor queries
    temperature: 0.8              # Slightly more creative for alternatives
    max_tokens: 500

  analysis:
    model: "gemini/gemini-2.5-flash"  # Model for analyzing results
    temperature: 0.3              # More deterministic for analysis
    max_tokens: 1500              # Analysis can be detailed

  synthesis:
    model: "gemini/gemini-2.5-flash"  # Model for synthesizing final answers
    temperature: 0.5              # Balanced for coherent narratives
    max_tokens: 2000              # Final answers can be long

  code_generation:
    model: "gemini/gemini-2.5-flash"  # Model for generating analysis code
    temperature: 0.2              # Very deterministic for code
    max_tokens: 1000              # Code is typically concise

# ============================================================================
# Execution Configuration
# ============================================================================
execution:
  max_concurrent: 10              # Max parallel API calls (balance speed/rate limits)
  max_refinements: 2              # Max query refinement iterations
  default_result_limit: 20        # Default results per database (increased from 10 per Codex recommendation)
  enable_adaptive_analysis: true  # Enable Ditto-style code generation
  enable_auto_refinement: true    # Enable automatic query refinement

# ============================================================================
# Per-Integration Result Limits
# ============================================================================
# Override default_result_limit for specific integrations
# Higher limits for sources with high-quality data or large API capacity
integration_limits:
  clearancejobs: 20      # 20 jobs per call (Playwright scraping)
  usajobs: 100           # USAJobs API supports 100 results/page
  brave_search: 20       # Free tier limit
  sam: 10                # SAM.gov can be slow, keep lower
  dvids: 50              # DVIDS API max is 50
  twitter: 20            # Default Twitter limit
  reddit: 20             # Default Reddit limit
  discord: 20            # Default Discord limit
  # Default: use execution.default_result_limit if integration not listed

# ============================================================================
# Timeout Configuration (all in seconds)
# ============================================================================
timeouts:
  api_request: 30                 # HTTP requests to external APIs
  llm_request: 180                # LLM API calls (3 minutes)
  code_execution: 30              # Generated code execution
  total_search: 300               # Total time for complete search (5 min)

# ============================================================================
# Database-Specific Settings
# ============================================================================
# Feature Flags: Set enabled: false to disable specific integrations
# This allows instant rollback if an integration breaks or causes issues
databases:
  # Government Data Sources
  sam:
    enabled: true
    timeout: 30                   # SAM.gov can be slow
    default_date_range_days: 60   # Default lookback period

  usajobs:
    enabled: true
    timeout: 20
    results_per_page: 100         # USAJobs API max

  dvids:
    enabled: true
    timeout: 20
    default_date_range_days: 90   # Military media archives
    origin: null                  # Set to registered domain if API key has origin restrictions (e.g., "https://example.com")

  clearancejobs:
    enabled: true
    requires_puppeteer: true      # Needs browser automation
    timeout: 45                   # Longer for Puppeteer operations

  fbi_vault:
    enabled: true
    timeout: 30                   # FBI Vault scraping can be slow

  congress:
    enabled: true
    timeout: 20                   # Congress.gov API is fast
    default_congress: 118         # 118th Congress (2023-2025)
    default_limit: 100            # Default results per query

  sec_edgar:
    enabled: true
    timeout: 15                   # SEC EDGAR APIs are fast
    max_results_per_query: 100    # Default limit for search results
    rate_limit_per_second: 10     # SEC enforces 10 requests/second
    # User email for User-Agent header (loaded from .env)
    # Required: SEC_EDGAR_USER_EMAIL (e.g., yourname@domain.com)
    user_email: ${SEC_EDGAR_USER_EMAIL}

  # Social Media & Community Sources
  discord:
    enabled: true
    timeout: 5                    # Local file search, fast

  twitter:
    enabled: true                 # Requires twitterexplorer_sigint
    timeout: 20

  reddit:
    enabled: true                 # Requires PRAW (installed)
    timeout: 15
    # Credentials loaded from environment variables (.env file)
    # Required: REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USERNAME, REDDIT_PASSWORD
    client_id: ${REDDIT_CLIENT_ID}
    client_secret: ${REDDIT_CLIENT_SECRET}
    username: ${REDDIT_USERNAME}
    password: ${REDDIT_PASSWORD}
    user_agent: "SIGINT_Platform/1.0"

  # Web Search & News
  brave_search:
    enabled: true
    timeout: 10

  newsapi:
    enabled: true
    timeout: 10
    max_results_per_query: 100    # NewsAPI max per request
    rate_limit_daily: 100          # Free tier: 100 requests/day
    max_age_days: 30               # Free tier: articles up to 1 month old
    # API key loaded from .env
    # Required: NEWSAPI_API_KEY
    api_key: ${NEWSAPI_API_KEY}

# ============================================================================
# Rate Limiting Strategies (Per-Source)
# ============================================================================
# Configure how different sources handle rate limits
rate_limiting:
  # Circuit breaker sources (skip after first 429)
  # These sources are known to have long-lasting rate limits (minutes to hours)
  # Once rate-limited, they will be skipped for remaining tasks in the session
  circuit_breaker_sources:
    - "SAM.gov"           # Known long rate limits (hours)

  # Sources that should never be skipped (always retry)
  # These sources are too critical to skip even if rate-limited
  critical_always_retry:
    - "USAJobs"           # Federal jobs - too important to skip

  # Global circuit breaker timeout (minutes to keep source blocked)
  # After this time, rate-limited sources may be retried
  # Note: SAM.gov rate limits typically outlast research sessions
  circuit_breaker_cooldown_minutes: 60

# ============================================================================
# Provider Fallback (LiteLLM Feature)
# ============================================================================
# If primary model fails, automatically try fallback models
# This provides resilience when Gemini has 503 overload errors
provider_fallback:
  enabled: true                   # Enabled for Gemini fallback chain
  fallback_models:
    - "gemini/gemini-2.5-flash-lite"  # Fallback 1: Lighter Gemini model
    - "gemini/gemini-2.0-flash-exp"   # Fallback 2: Older stable Gemini

# ============================================================================
# Cost Management
# ============================================================================
cost_management:
  max_cost_per_query: 0.50        # USD - abort if query exceeds this
  track_costs: true               # Log LLM API costs
  warn_on_expensive_queries: true # Warn if query > 50% of max

# ============================================================================
# Research Flow Configuration (Supervisor/Scoping/HITL)
# ============================================================================
research:
  # Deep research execution limits
  deep_research:
    max_tasks: 15                 # Maximum tasks to execute (prevents infinite loops)
    max_retries_per_task: 2       # Max retries for failed tasks
    max_time_minutes: 120         # Maximum total investigation time (minutes)
    min_results_per_task: 3       # Minimum results to consider task successful
    max_concurrent_tasks: 4       # Max parallel tasks (1=sequential, 3-5=parallel)
    task_timeout_seconds: 1800    # Per-task timeout (30 min) - Backstop for infinite retry loops, primary timeout at LLM call level (60s)
    max_follow_ups_per_task: null # Max follow-ups per task (null=unlimited, N=cap per task) - LLM decides 0-N based on coverage quality
    # Phase 5: Removed min_coverage_for_followups (was 95%, never triggered)
    # LLM decides if follow-ups needed based on qualitative gap assessment

  # ============================================================================
  # Phase 4: Manager-Agent Architecture (Task Prioritization + Saturation)
  # ============================================================================
  # Expert investigator mode - Manager LLM prioritizes tasks and detects saturation
  # Enables "run until done" behavior with intelligent stopping
  manager_agent:
    enabled: true                     # Enable task prioritization (Phase 4A) - DEFAULT: true

    # Saturation detection (Phase 4B)
    saturation_detection: true        # Check for research saturation - DEFAULT: true
    saturation_check_interval: 3      # Check every N tasks (avoid over-calling) - DEFAULT: 3
    saturation_confidence_threshold: 70  # Stop if saturation confidence >= % - DEFAULT: 70 (70-90 recommended)
    allow_saturation_stop: true       # Stop when saturated even if under max_tasks - DEFAULT: true
                                       # If false: Saturation logged but ignored (always run to max_tasks)

    # Reprioritization behavior
    reprioritize_after_task: true     # Reprioritize queue after each completion - DEFAULT: true
                                       # If false: Only prioritize initial queue (FIFO for follow-ups)

    # Cost notes:
    # - Prioritization: 1 LLM call per reprioritization (if 14 tasks → ~14 calls @ $0.005 = $0.07)
    # - Saturation: 1 LLM call every 3 tasks (if 14 tasks → ~4 calls @ $0.005 = $0.02)
    # - Total Phase 4 cost: ~$0.10 for 14-task research (minimal overhead)

  # Feature flags (start disabled, enable after validation)
  enable_scoping: false           # Enable ScopingAgent (query clarification + planning)
  enable_supervisor: false        # Enable ResearchSupervisor (task decomposition + routing)
  enable_hitl: false              # Enable Human-in-the-Loop approval

  # Scoping parameters
  max_subtasks: 5                 # Max sub-questions per research brief
  auto_clarify_threshold: 0.7     # Confidence score (0-1) to skip clarification (higher = more clarification)

  # Model roles (Phase 1: all use same model, Phase 2: specialize)
  model_roles:
    scoping: "gemini/gemini-2.5-flash"
    research: "gemini/gemini-2.5-flash"
    summarization: "gemini/gemini-2.5-flash"  # Phase 2: consider lighter model for cost savings
    synthesis: "gemini/gemini-2.5-flash"

  # ============================================================================
  # Phase 3: Hypothesis Branching Configuration
  # ============================================================================
  # IMPORTANT: Hypothesis branching increases LLM cost by 3-3.75x vs traditional task decomposition
  # Enable only if you prioritize quality over cost and have budget for comprehensive exploration
  hypothesis_branching:
    mode: "execution"             # OPERATION MODE: "off" | "planning" | "execution"
                                  # - off: Traditional task decomposition (no hypotheses)
                                  # - planning: Generate hypotheses but don't execute (Phase 3A - shows in report only)
                                  # - execution: Generate AND execute hypotheses (Phase 3B - runs searches)
                                  # Default: "execution" (changed 2025-11-18 per user request)

    # DEPRECATED: Use "mode" instead
    # enabled: false              # Legacy config - auto-upgrades to mode: "planning" | "off"

    max_hypotheses_per_task: 5   # CEILING (not target) - LLM decides actual count (1-5)
                                  # Simple queries: 1-2 hypotheses
                                  # Factual queries: 2-3 hypotheses
                                  # Speculative queries: 3-5 hypotheses
                                  # Higher value = more comprehensive coverage but higher cost

    # COST BREAKDOWN BY MODE:
    # - mode: "off" → 0 additional cost (baseline)
    # - mode: "planning" → +1 LLM call per task (~25-50% increase)
    # - mode: "execution" → +1 generation + N query gen calls per task (~3-3.75x baseline)
    #
    # Phase 3B (execution mode) costs:
    # - Hypothesis generation: 1 LLM call per task
    # - Query generation: 1 LLM call per hypothesis (up to max_hypotheses_per_task)
    # - Total Phase 3B cost: 3-3.75x traditional approach (documented in investigation)

    # ============================================================================
    # Phase 3C: Coverage Assessment Configuration
    # ============================================================================
    # Adaptive stopping based on coverage analysis (Phase 3C)
    # Only applies when mode: "execution"

    coverage_mode: true             # EXECUTION STRATEGY: false | true
                                     # - false: Parallel execution, all hypotheses run (Phase 3B - faster)
                                     # - true: Sequential execution with adaptive stopping (Phase 3C - adaptive)
                                     # Changed to true 2025-11-19 to enable Phase 3C coverage assessment

    max_hypotheses_to_execute: 5     # HARD CEILING: Maximum hypotheses to execute (never exceed)
                                     # Same as max_hypotheses_per_task unless you want different limit for execution
                                     # LLM can stop early but will never exceed this limit
                                     # NOTE: Task timeout controlled by deep_research.task_timeout_seconds (600s)

    # Phase 3C (coverage_mode: true) behavior:
    # - Executes hypotheses sequentially (one-by-one, not parallel)
    # - After each hypothesis, LLM assesses coverage based on:
    #   * New vs duplicate results percentage
    #   * New vs duplicate entities discovered
    #   * Time budget remaining
    #   * Hard ceiling on max hypotheses
    # - LLM decides: continue (explore next hypothesis) or stop (sufficient coverage)
    # - Stops early if LLM determines coverage is sufficient, saving time and LLM calls
    # - Falls back to all hypotheses if LLM keeps saying "continue" (respects hard ceilings)

# ============================================================================
# Logging Configuration
# ============================================================================
logging:
  level: "INFO"                   # DEBUG, INFO, WARNING, ERROR
  log_llm_calls: true             # Log all LLM API calls
  log_api_calls: true             # Log all database API calls
  log_file: "research.log"        # Log file path (null for stdout only)
  log_to_stdout: true             # Also print to console
