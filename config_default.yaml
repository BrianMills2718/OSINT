# Default Configuration for AI Research System
# This file contains sensible defaults for all configurable parameters.
# Copy to config.yaml and customize as needed.

# ============================================================================
# LLM Configuration
# ============================================================================
# LiteLLM supports 100+ providers - just change the model name!
# Examples:
#   OpenAI GPT-5: "gpt-5", "gpt-5-mini", "gpt-5-nano" (recommended: gpt-5-nano for cost)
#   OpenAI GPT-4: "gpt-4o", "gpt-4o-mini"
#   Anthropic: "claude-3-5-sonnet-20241022", "claude-3-opus-20240229"
#   Google: "gemini/gemini-2.0-flash-exp", "gemini/gemini-1.5-pro"
#   Local: "ollama/llama3", "ollama/mistral"
#
# Model Recommendations by Cost/Performance:
#   - gpt-5-nano: Lowest cost, good for simple queries (~10x cheaper than gpt-5-mini)
#   - gpt-5-mini: Best balance of cost/performance (current default)
#   - gpt-5: Highest quality, highest cost

llm:
  # Default model for all operations (can be overridden per-operation)
  default_model: "gemini/gemini-2.5-flash"

  # Operation-specific model configurations
  # Each can have: model, temperature, max_tokens

  query_generation:
    model: "gemini/gemini-2.5-flash"  # Model for generating database queries
    temperature: 0.7              # Balanced creativity/precision
    max_tokens: 500               # Queries are typically short

  refinement:
    model: "gemini/gemini-2.5-flash"  # Model for refining poor queries
    temperature: 0.8              # Slightly more creative for alternatives
    max_tokens: 500

  analysis:
    model: "gemini/gemini-2.5-flash"  # Model for analyzing results
    temperature: 0.3              # More deterministic for analysis
    max_tokens: 1500              # Analysis can be detailed

  synthesis:
    model: "gemini/gemini-2.5-flash"  # Model for synthesizing final answers
    temperature: 0.5              # Balanced for coherent narratives
    max_tokens: 2000              # Final answers can be long

  code_generation:
    model: "gemini/gemini-2.5-flash"  # Model for generating analysis code
    temperature: 0.2              # Very deterministic for code
    max_tokens: 1000              # Code is typically concise

# ============================================================================
# Execution Configuration
# ============================================================================
execution:
  max_concurrent: 10              # Max parallel API calls (balance speed/rate limits)
  max_refinements: 2              # Max query refinement iterations
  default_result_limit: 20        # Default results per database (increased from 10 per Codex recommendation)
  enable_adaptive_analysis: true  # Enable Ditto-style code generation
  enable_auto_refinement: true    # Enable automatic query refinement

# ============================================================================
# Per-Integration Result Limits
# ============================================================================
# Override default_result_limit for specific integrations
# Higher limits for sources with high-quality data or large API capacity
integration_limits:
  clearancejobs: 20      # 20 jobs per call (Playwright scraping)
  usajobs: 100           # USAJobs API supports 100 results/page
  brave_search: 20       # Free tier limit
  sam: 10                # SAM.gov can be slow, keep lower
  dvids: 50              # DVIDS API max is 50
  twitter: 20            # Default Twitter limit
  reddit: 20             # Default Reddit limit
  discord: 20            # Default Discord limit
  # Default: use execution.default_result_limit if integration not listed

# ============================================================================
# Timeout Configuration (all in seconds)
# ============================================================================
timeouts:
  api_request: 30                 # HTTP requests to external APIs
  llm_request: 60                 # LLM API calls
  code_execution: 30              # Generated code execution
  total_search: 300               # Total time for complete search (5 min)

# ============================================================================
# Database-Specific Settings
# ============================================================================
# Feature Flags: Set enabled: false to disable specific integrations
# This allows instant rollback if an integration breaks or causes issues
databases:
  # Government Data Sources
  sam:
    enabled: true
    timeout: 30                   # SAM.gov can be slow
    default_date_range_days: 60   # Default lookback period

  usajobs:
    enabled: true
    timeout: 20
    results_per_page: 100         # USAJobs API max

  dvids:
    enabled: true
    timeout: 20
    default_date_range_days: 90   # Military media archives
    origin: null                  # Set to registered domain if API key has origin restrictions (e.g., "https://example.com")

  clearancejobs:
    enabled: true
    requires_puppeteer: true      # Needs browser automation
    timeout: 45                   # Longer for Puppeteer operations

  fbi_vault:
    enabled: true
    timeout: 30                   # FBI Vault scraping can be slow

  # Social Media & Community Sources
  discord:
    enabled: true
    timeout: 5                    # Local file search, fast

  twitter:
    enabled: true                 # Requires twitterexplorer_sigint
    timeout: 20

  reddit:
    enabled: true                 # Requires PRAW (installed)
    timeout: 15
    # Credentials loaded from environment variables (.env file)
    # Required: REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USERNAME, REDDIT_PASSWORD
    client_id: ${REDDIT_CLIENT_ID}
    client_secret: ${REDDIT_CLIENT_SECRET}
    username: ${REDDIT_USERNAME}
    password: ${REDDIT_PASSWORD}
    user_agent: "SIGINT_Platform/1.0"

  # Web Search
  brave_search:
    enabled: true
    timeout: 10

# ============================================================================
# Rate Limiting Strategies (Per-Source)
# ============================================================================
# Configure how different sources handle rate limits
rate_limiting:
  # Circuit breaker sources (skip after first 429)
  # These sources are known to have long-lasting rate limits (minutes to hours)
  # Once rate-limited, they will be skipped for remaining tasks in the session
  circuit_breaker_sources:
    - "SAM.gov"           # Known long rate limits (hours)

  # Sources that should never be skipped (always retry)
  # These sources are too critical to skip even if rate-limited
  critical_always_retry:
    - "USAJobs"           # Federal jobs - too important to skip

  # Global circuit breaker timeout (minutes to keep source blocked)
  # After this time, rate-limited sources may be retried
  # Note: SAM.gov rate limits typically outlast research sessions
  circuit_breaker_cooldown_minutes: 60

# ============================================================================
# Provider Fallback (LiteLLM Feature)
# ============================================================================
# If primary model fails, automatically try fallback models
# This provides resilience when Gemini has 503 overload errors
provider_fallback:
  enabled: true                   # Enabled for Gemini fallback chain
  fallback_models:
    - "gemini/gemini-2.5-flash-lite"  # Fallback 1: Lighter Gemini model
    - "gemini/gemini-2.0-flash-exp"   # Fallback 2: Older stable Gemini

# ============================================================================
# Cost Management
# ============================================================================
cost_management:
  max_cost_per_query: 0.50        # USD - abort if query exceeds this
  track_costs: true               # Log LLM API costs
  warn_on_expensive_queries: true # Warn if query > 50% of max

# ============================================================================
# Research Flow Configuration (Supervisor/Scoping/HITL)
# ============================================================================
research:
  # Feature flags (start disabled, enable after validation)
  enable_scoping: false           # Enable ScopingAgent (query clarification + planning)
  enable_supervisor: false        # Enable ResearchSupervisor (task decomposition + routing)
  enable_hitl: false              # Enable Human-in-the-Loop approval

  # Scoping parameters
  max_subtasks: 5                 # Max sub-questions per research brief
  auto_clarify_threshold: 0.7     # Confidence score (0-1) to skip clarification (higher = more clarification)

  # Model roles (Phase 1: all use same model, Phase 2: specialize)
  model_roles:
    scoping: "gemini/gemini-2.5-flash"
    research: "gemini/gemini-2.5-flash"
    summarization: "gemini/gemini-2.5-flash"  # Phase 2: consider lighter model for cost savings
    synthesis: "gemini/gemini-2.5-flash"

  # ============================================================================
  # Phase 3: Hypothesis Branching Configuration
  # ============================================================================
  # IMPORTANT: Hypothesis branching increases LLM cost by 3-3.75x vs traditional task decomposition
  # Enable only if you prioritize quality over cost and have budget for comprehensive exploration
  hypothesis_branching:
    enabled: false                # FEATURE TOGGLE: Set true to enable hypothesis branching (Phase 3A foundation only)
                                  # Default: false (use traditional task decomposition)
                                  # When enabled: Each task generates 1-5 investigative hypotheses

    max_hypotheses_per_task: 5   # CEILING (not target) - LLM decides actual count (1-5)
                                  # Simple queries: 1-2 hypotheses
                                  # Factual queries: 2-3 hypotheses
                                  # Speculative queries: 3-5 hypotheses
                                  # Higher value = more comprehensive coverage but higher cost

    # COST WARNING: Phase 3A (hypothesis generation) adds ~1 LLM call per task
    # Current cost per task: ~2-4 LLM calls (source selection, relevance evaluation, entity extraction)
    # With Phase 3A enabled: ~3-5 LLM calls per task (adds hypothesis generation)
    # Cost increase: ~25-50% vs traditional approach
    #
    # Phase 3B-3D (not yet implemented) would add additional costs:
    # - Hypothesis execution: Per-hypothesis query generation
    # - Coverage assessment: LLM decides when to stop exploring
    # - Total Phase 3 cost: 3-3.75x traditional approach (documented in investigation)

# ============================================================================
# Logging Configuration
# ============================================================================
logging:
  level: "INFO"                   # DEBUG, INFO, WARNING, ERROR
  log_llm_calls: true             # Log all LLM API calls
  log_api_calls: true             # Log all database API calls
  log_file: "research.log"        # Log file path (null for stdout only)
  log_to_stdout: true             # Also print to console
