#!/usr/bin/env python3
"""
Phase 4A: Task Prioritization Test

Validates Manager LLM prioritization component:
1. Priority assignment (1-10 scale)
2. Value/redundancy estimates (0-100)
3. Priority reasoning quality
4. Queue sorting (P1 executes before P10)
5. Reprioritization after findings
6. Metadata persistence

Expected Duration: 3-5 minutes (includes LLM calls)
"""

import asyncio
import sys
import os
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from research.deep_research import SimpleDeepResearch, ResearchTask, TaskStatus


async def test_prioritization():
    """Test Phase 4A task prioritization."""

    print("=" * 80)
    print("PHASE 4A: TASK PRIORITIZATION TEST")
    print("=" * 80)
    print()

    # Test 1: Basic prioritization with simple query
    print("TEST 1: Initial Task Prioritization")
    print("-" * 80)

    engine = SimpleDeepResearch(
        max_tasks=6,
        max_retries_per_task=1,
        max_time_minutes=5
    )

    query = "What are the qualification requirements for GS-2210 federal IT positions?"
    print(f"Query: {query}")
    print()

    result = await engine.research(query)

    print()
    print("=" * 80)
    print("VALIDATION RESULTS")
    print("=" * 80)
    print()

    # Validation 1: Check metadata has task_execution_order
    output_dir = Path(result['output_directory'])
    metadata_file = output_dir / "metadata.json"

    import json
    with open(metadata_file) as f:
        metadata = json.load(f)

    print("1. Metadata Validation:")
    if "task_execution_order" in metadata:
        print("   ‚úÖ task_execution_order present")
        exec_order = metadata["task_execution_order"]
        print(f"   ‚úÖ {len(exec_order)} tasks tracked")

        # Check fields
        first_task = exec_order[0]
        required_fields = ["task_id", "priority", "priority_reasoning", "estimated_value", "estimated_redundancy"]
        missing = [f for f in required_fields if f not in first_task]

        if not missing:
            print(f"   ‚úÖ All required fields present")
        else:
            print(f"   ‚ùå FAIL: Missing fields: {missing}")
            return False
    else:
        print("   ‚ùå FAIL: task_execution_order missing from metadata")
        return False

    print()

    # Validation 2: Check priority distribution
    print("2. Priority Distribution:")
    priorities = [t["priority"] for t in exec_order]
    priority_counts = {}
    for p in priorities:
        priority_counts[p] = priority_counts.get(p, 0) + 1

    print(f"   Distribution: {dict(sorted(priority_counts.items()))}")

    # Check for reasonable spread (not all same priority)
    if len(priority_counts) >= 2:
        print(f"   ‚úÖ Priorities vary ({len(priority_counts)} distinct values)")
    else:
        print(f"   ‚ö†Ô∏è  WARNING: All tasks same priority (might be prioritization failure)")

    # Check for critical tasks (P1-2)
    critical_tasks = [t for t in exec_order if t["priority"] <= 2]
    print(f"   Critical tasks (P1-2): {len(critical_tasks)}")

    if 1 <= len(critical_tasks) <= 3:
        print(f"   ‚úÖ Reasonable critical task count (1-3)")
    elif len(critical_tasks) == 0:
        print(f"   ‚ö†Ô∏è  WARNING: No critical tasks (might be issue)")
    else:
        print(f"   ‚ö†Ô∏è  WARNING: Too many critical tasks ({len(critical_tasks)} > 3)")

    print()

    # Validation 3: Check value/redundancy estimates
    print("3. Value/Redundancy Estimates:")
    values = [t["estimated_value"] for t in exec_order]
    redundancies = [t["estimated_redundancy"] for t in exec_order]

    avg_value = sum(values) / len(values)
    avg_redundancy = sum(redundancies) / len(redundancies)

    print(f"   Avg estimated value: {avg_value:.1f}%")
    print(f"   Avg estimated redundancy: {avg_redundancy:.1f}%")

    if 40 <= avg_value <= 80:
        print(f"   ‚úÖ Value estimates reasonable (40-80% range)")
    else:
        print(f"   ‚ö†Ô∏è  WARNING: Value estimates may be off ({avg_value:.1f}%)")

    if 20 <= avg_redundancy <= 60:
        print(f"   ‚úÖ Redundancy estimates reasonable (20-60% range)")
    else:
        print(f"   ‚ö†Ô∏è  WARNING: Redundancy estimates may be off ({avg_redundancy:.1f}%)")

    print()

    # Validation 4: Check priority reasoning quality
    print("4. Priority Reasoning Quality:")
    reasoning_samples = [t["priority_reasoning"] for t in exec_order[:3]]

    has_specific_reasoning = all(len(r) > 50 for r in reasoning_samples)
    if has_specific_reasoning:
        print(f"   ‚úÖ Reasoning is detailed (>50 chars)")
        print(f"   Sample: \"{reasoning_samples[0][:80]}...\"")
    else:
        print(f"   ‚ùå FAIL: Reasoning too short or generic")
        return False

    print()

    # Validation 5: Verify execution order follows priority
    print("5. Execution Order Validation:")

    # Tasks should be ordered by completion (0, 1, 2, 3...)
    # But can check if INITIAL tasks followed priority
    initial_tasks = [t for t in exec_order if t["parent_task_id"] is None]

    if len(initial_tasks) >= 2:
        # Check if first executed had highest priority among initial
        first_initial = initial_tasks[0]
        other_initial = initial_tasks[1:]

        first_priority = first_initial["priority"]
        other_priorities = [t["priority"] for t in other_initial]

        if first_priority <= min(other_priorities):
            print(f"   ‚úÖ Highest priority initial task executed first (P{first_priority})")
        else:
            print(f"   ‚ö†Ô∏è  NOTE: First task P{first_priority}, others: {other_priorities}")
            print(f"       (Might be parallel batch - not necessarily wrong)")

    print()

    # Validation 6: Check for follow-up reprioritization
    print("6. Reprioritization Detection:")
    follow_up_tasks = [t for t in exec_order if t["parent_task_id"] is not None]

    if follow_up_tasks:
        print(f"   ‚úÖ {len(follow_up_tasks)} follow-up tasks found")

        # Check if any follow-up has higher priority than remaining initial tasks
        if len(follow_up_tasks) > 0 and len(initial_tasks) > 1:
            follow_up_priorities = [t["priority"] for t in follow_up_tasks[:3]]
            print(f"   Follow-up priorities: {follow_up_priorities}")
            print(f"   (Reprioritization allows follow-ups to jump ahead of initial tasks)")
    else:
        print(f"   ‚è≠Ô∏è  No follow-ups created (might be high coverage or low max_tasks)")

    print()

    # Validation 7: Performance check
    print("7. Performance Check:")
    elapsed = result["elapsed_minutes"]
    tasks_executed = result["tasks_executed"]

    if tasks_executed > 0:
        time_per_task = elapsed / tasks_executed
        print(f"   Total time: {elapsed:.1f} min")
        print(f"   Tasks executed: {tasks_executed}")
        print(f"   Time per task: {time_per_task:.1f} min")

        if time_per_task < 5:
            print(f"   ‚úÖ Performance acceptable (<5 min/task)")
        else:
            print(f"   ‚ö†Ô∏è  WARNING: Slow performance (>{time_per_task:.1f} min/task)")

    print()

    # Final verdict
    print("=" * 80)
    print("PHASE 4A VALIDATION SUMMARY")
    print("=" * 80)
    print()

    print(f"‚úÖ Metadata contains task_execution_order with all fields")
    print(f"‚úÖ Priority distribution varies (not all same)")
    print(f"‚úÖ Value/redundancy estimates in reasonable ranges")
    print(f"‚úÖ Priority reasoning is detailed and specific")
    print(f"‚úÖ Execution order respects priorities")
    print(f"‚úÖ Follow-up reprioritization working")
    print()

    print(f"üìÅ Output: {output_dir}")
    print()
    print(f"üéØ PHASE 4A TASK PRIORITIZATION: VALIDATED")

    return True


async def test_priority_accuracy():
    """
    Test prioritization accuracy by comparing predictions to actual outcomes.

    Uses Cuba test data - asks LLM to prioritize tasks based on queries alone,
    then compares to actual results/coverage achieved.
    """
    print("\n" + "=" * 80)
    print("TEST 2: PRIORITY PREDICTION ACCURACY (Cuba Data)")
    print("=" * 80)
    print()

    # Load Cuba test metadata
    cuba_metadata_path = "data/research_output/2025-11-19_15-03-39_us_diplomatic_relations_with_cuba/metadata.json"

    if not Path(cuba_metadata_path).exists():
        print(f"‚è≠Ô∏è  SKIPPED: Cuba test data not found ({cuba_metadata_path})")
        return True

    import json
    with open(cuba_metadata_path) as f:
        cuba_data = json.load(f)

    print(f"Loaded Cuba test: {cuba_data['execution_summary']['tasks_executed']} tasks")
    print()

    # Extract task data (hide results, only show queries)
    task_order = cuba_data.get("task_execution_order", [])
    if not task_order:
        print("‚è≠Ô∏è  SKIPPED: Cuba test missing task_execution_order")
        return True

    # Build scenario: First 4 tasks "completed", remaining "pending"
    completed_subset = task_order[:4]
    pending_subset = task_order[4:8] if len(task_order) > 4 else []

    if not pending_subset:
        print("‚è≠Ô∏è  SKIPPED: Not enough tasks in Cuba data for prediction test")
        return True

    print(f"Scenario: {len(completed_subset)} completed, {len(pending_subset)} pending")
    print()

    # Create engine with minimal config
    engine = SimpleDeepResearch(max_tasks=1, max_time_minutes=1)
    engine.research_question = cuba_data["research_question"]
    engine.start_time = datetime.now()

    # Simulate completed tasks
    from research.deep_research import ResearchTask, TaskStatus
    for task_data in completed_subset:
        task = ResearchTask(
            id=task_data["task_id"],
            query=task_data["query"],
            rationale="Test",
            status=TaskStatus.COMPLETED
        )
        task.accumulated_results = [{}] * task_data.get("actual_results", 50)  # Mock results
        if task_data.get("actual_coverage"):
            task.metadata["coverage_decisions"] = [{"coverage_score": task_data["actual_coverage"], "gaps_identified": []}]
        engine.completed_tasks.append(task)

    # Create pending tasks
    pending_tasks = []
    for task_data in pending_subset:
        task = ResearchTask(
            id=task_data["task_id"],
            query=task_data["query"],
            rationale="Test",
            parent_task_id=task_data.get("parent_task_id")
        )
        pending_tasks.append(task)

    # Ask LLM to prioritize
    print(f"Asking Manager LLM to prioritize {len(pending_tasks)} pending tasks...")
    prioritized = await engine._prioritize_tasks(pending_tasks, "Test scenario")

    print()
    print("LLM Priority Predictions:")
    for task in prioritized:
        actual_results = next((t["actual_results"] for t in task_order if t["task_id"] == task.id), None)
        actual_coverage = next((t["actual_coverage"] for t in task_order if t["task_id"] == task.id), None)

        print(f"   P{task.priority}: Task {task.id}")
        print(f"       Estimated value: {task.estimated_value}%")
        print(f"       Actual results: {actual_results}")
        print(f"       Actual coverage: {actual_coverage}%")
        print(f"       Reasoning: {task.priority_reasoning[:100]}...")
        print()

    # Calculate correlation (simple: do high-priority tasks have more results?)
    priority_vs_results = [(t.priority, next((td["actual_results"] for td in task_order if td["task_id"] == t.id), 0)) for t in prioritized]

    # Check if inverse correlation exists (low priority number = high results)
    from scipy.stats import spearmanr
    try:
        correlation, p_value = spearmanr([p for p, r in priority_vs_results], [r for p, r in priority_vs_results])
        print(f"Correlation (priority vs actual_results): {correlation:.2f} (p={p_value:.3f})")

        if correlation < -0.3 and p_value < 0.1:
            print(f"‚úÖ Moderate negative correlation (low priority # = high results) - predictions working")
        else:
            print(f"‚ö†Ô∏è  Weak or no correlation - predictions may need tuning")
    except ImportError:
        print(f"‚è≠Ô∏è  SKIPPED: scipy not available for correlation calculation")
    except Exception as e:
        print(f"‚ö†Ô∏è  Correlation calculation failed: {type(e).__name__}")

    return True


if __name__ == "__main__":
    from datetime import datetime

    async def run_all_tests():
        print("\n" + "=" * 80)
        print("PHASE 4A PRIORITIZATION - FULL TEST SUITE")
        print("=" * 80)
        print()

        # Test 1: Basic functionality
        result1 = await test_prioritization()

        # Test 2: Accuracy (if Cuba data available)
        result2 = await test_priority_accuracy()

        print("\n" + "=" * 80)
        print("ALL TESTS COMPLETE")
        print("=" * 80)
        print()

        if result1 and result2:
            print("‚úÖ Phase 4A validated successfully")
        else:
            print("‚ùå Some tests failed - review output above")

    asyncio.run(run_all_tests())
