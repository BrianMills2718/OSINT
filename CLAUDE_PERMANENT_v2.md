# CLAUDE.md - Permanent Section (v2 - Condensed)

---

## PURPOSE

**CRITICAL**: This file guides EVERY Claude Code API call.

**Structure**:
- **PERMANENT**: Core principles (rarely changes)
- **TEMPORARY**: Current tasks (updated frequently)

**Source Files**: CLAUDE_PERMANENT_v2.md, CLAUDE_TEMP.md, REGENERATE_CLAUDE.md

**When to Update**:
- **TEMPORARY only**: Tasks complete, blockers change, next actions shift
- **PERMANENT**: Major architecture changes, new systematic failure patterns

---

## VISION

**SigInt Platform** - AI-Powered Investigative Journalism
- 15+ government/social/document sources
- Automated Boolean monitoring + email alerts
- AI analysis (summaries, entity extraction, timelines)
- Team collaboration workspaces

**Current**: ~60% complete, 4 databases working
**Timeline**: 3-6 months to production

See: INVESTIGATIVE_PLATFORM_VISION.md (75 pages)

---

## CORE PRINCIPLES (MANDATORY)

### 1. ADVERSARIAL TESTING

**Assume all tests will fail. Prove they didn't.**

- Before claiming success, actively try to break it
- Look for what you CANNOT prove
- Document limitations > fabricate success

âŒ WRONG: "Integration complete! Everything working!"
âœ… RIGHT: "[PASS] Query gen: 5 jobs returned | [UNVERIFIED] Parallel execution, error handling | [LIMITATION] Slow on WSL2 (8s vs 5s)"

### 2. EVIDENCE HIERARCHY

**Only these count**:
1. **Command output** - Highest proof
2. **Error messages** - Proof of failure
3. **User execution** - Ultimate proof

**NOT evidence**:
- "Should work because..." (reasoning â‰  proof)
- "Code looks correct..." (existence â‰  proof)
- "Tests passed" without output (claim â‰  proof)

### 3. FORBIDDEN/REQUIRED LANGUAGE

**NEVER**: "Success!", "Done!", "EXCELLENT!", "Working!", "Perfect!", âœ…ðŸŽ‰âœ¨
**ALWAYS**: "[PASS]", "[FAIL]", "[BLOCKED]", "Test passed:", "Limitation found:", "Unverified:"

### 4. ANTI-LYING CHECKLIST (BEFORE REPORTING)

1. â“ Read COMPLETE output, not just success messages?
2. â“ Count failures AND successes?
3. â“ About to use âœ… emoji? â†’ Use [PASS]/[FAIL] instead
4. â“ Ignoring/downplaying failures? â†’ Lead with failures
5. â“ Cherry-picking good parts? â†’ Report failures first

**Format**: `[FAIL] X: error | [FAIL] Y: error | [PASS] Z: success`

### 5. DISCOVERY BEFORE BUILDING

**BEFORE building, check if it exists**:
1. Read STATUS.md - what's [PASS]?
2. Read CLAUDE.md TEMPORARY - current task?
3. Check directory - does it already exist?
4. Test existing entry points - does it work?

**Why**: Prevents rebuilding existing infrastructure

### 6. CIRCUIT BREAKERS (HARD STOP)

**STOP immediately when**:
1. Import errors on entry points
2. 3+ consecutive timeouts
3. Scope drift (doing more than declared)
4. No evidence after 30 minutes
5. Circular work (repeating failed approach)
6. Config file not found
7. API quota/rate limit exceeded

### 7. FAIL-FAST AND LOUD

- Surface errors with full stack traces
- NEVER swallow exceptions
- Log: API calls, LLM queries, config loads, validation checks

### 8. NO LAZY IMPLEMENTATIONS

**Forbidden**: Mocks in production, fallbacks hiding errors, TODO pseudocode
**Required**: Real implementations or explicit documentation of gaps

---

## DIRECTORY STRUCTURE

```
sam_gov/
â”œâ”€â”€ CLAUDE.md, STATUS.md, PATTERNS.md
â”œâ”€â”€ core/                    # Research engines
â”œâ”€â”€ integrations/            # Data source adapters
â”‚   â”œâ”€â”€ government/          # SAM, DVIDS, USAJobs, ClearanceJobs
â”‚   â””â”€â”€ social/              # Twitter, Reddit, Discord
â”œâ”€â”€ apps/                    # User entry points (ai_research.py, unified_search_app.py)
â”œâ”€â”€ tests/                   # All test scripts
â”œâ”€â”€ research/                # Deep research workflows
â”œâ”€â”€ data/                    # Runtime storage (articles, exports, logs, monitors)
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ archive/YYYY-MM-DD/      # Archived code/docs
â”œâ”€â”€ .env, config.yaml        # Configuration (gitignored)
â””â”€â”€ llm_utils.py, config_loader.py
```

### Root Directory Discipline

**Only in Root** (~15 files):
- Core docs (9): CLAUDE.md, STATUS.md, PATTERNS.md, etc.
- Config (4-5): .env, .gitignore, requirements.txt, config*.yaml
- Core utils (2): llm_utils.py, config_loader.py

**Archive immediately**:
- Test scripts â†’ `tests/`
- *_COMPLETE.md, *_STATUS.md â†’ `archive/YYYY-MM-DD/docs/`
- *_PLAN.md â†’ `docs/` (active) or `archive/` (completed)
- *.log, temp_*.txt â†’ `data/logs/`

**If root >20 files**: STOP, archive excess, create README

### File Finding

- Current task â†’ CLAUDE.md TEMPORARY
- Is X working? â†’ STATUS.md
- How to implement Y? â†’ PATTERNS.md
- Why? â†’ INVESTIGATIVE_PLATFORM_VISION.md
- Archived? â†’ archive/YYYY-MM-DD/README.md

### Documentation Layers

1. **VISION** (INVESTIGATIVE_PLATFORM_VISION.md) - What we want
2. **ROADMAP** (ROADMAP.md) - How we plan it
3. **STATUS** (STATUS.md) - What actually works
4. **CLAUDE.md TEMPORARY** - What I'm doing now

**Before work**: Read TEMPORARY â†’ STATUS â†’ Test existing â†’ Only build if missing/broken

---

## ENVIRONMENT

### Python (.venv)

**MANDATORY**: ALL commands must activate `.venv` first

```bash
source .venv/bin/activate  # FIRST, EVERY TIME
python3 script.py          # Now uses .venv
```

**Circuit Breaker**: `ModuleNotFoundError: No module named 'playwright'`
â†’ STOP, activate `.venv`, rerun

### API Keys

**ALWAYS use python-dotenv**:

```python
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')
```

**NEVER**: Hardcode keys, commit .env, use os.environ without load_dotenv()

---

## CODE PATTERNS

### LLM Calls

```python
from llm_utils import acompletion
from dotenv import load_dotenv

load_dotenv()

response = await acompletion(
    model="gpt-5-mini",
    messages=[{"role": "user", "content": prompt}],
    response_format={"type": "json_schema", "json_schema": {...}}
)
```

**NEVER**: Call litellm directly, use max_tokens/max_output_tokens (breaks gpt-5)

### Database Integration

**Copy from**: `integrations/government/sam_integration.py`

**Required**:
```python
class NewIntegration(DatabaseIntegration):
    @property
    def metadata(self) -> DatabaseMetadata: ...
    async def is_relevant(self, question: str) -> bool: ...
    async def generate_query(self, question: str) -> Optional[Dict]: ...
    async def execute_search(self, params, key, limit) -> QueryResult: ...
```

**After creating**:
1. Save in `integrations/government/` or `social/`
2. Register in `integrations/registry.py`
3. Create `tests/test_newsource_live.py`
4. Test E2E: `python3 apps/ai_research.py "query"`
5. Update STATUS.md with [PASS]/[BLOCKED]

---

## TESTING

### Categories

1. **Unit**: Isolated, may mock, fast
2. **Integration**: Multiple components, real APIs
3. **E2E** (REQUIRED for success claims): User-facing entry points, no mocks

### Checklist (Before Claiming Success)

ALL must be true:
- [ ] Imports resolve
- [ ] Config loads from .env/config.yaml
- [ ] API calls succeed
- [ ] Results match format
- [ ] Errors handled gracefully
- [ ] User can execute without assistance
- [ ] No timeouts
- [ ] Logging works
- [ ] Cost tracking (if LLM)

**If ANY false â†’ NOT succeeded**

---

## KNOWN ISSUES

1. **gpt-5 models**: NEVER use max_tokens (breaks reasoning). Use llm_utils.acompletion()
2. **ClearanceJobs**: Official API broken. Use clearancejobs_playwright.py (slower but accurate)
3. **USAJobs**: Requires headers: `User-Agent: email`, `Authorization-Key: [key]`

---

## ESSENTIAL TOOLS

**Keep** (complex/critical):
- tests/test_all_four_databases.py
- tests/test_verification.py
- llm_utils.py, config_loader.py
- integrations/government/sam_integration.py

**Regenerate on-demand** (simple):
- Unit tests, CLI scripts, utilities

**Entry Points** (always keep):
- apps/ai_research.py
- apps/unified_search_app.py

---

## QUICK REFERENCE

```bash
source .venv/bin/activate  # ALWAYS FIRST

# Test entry points
python3 apps/ai_research.py "query"
python3 tests/test_verification.py
streamlit run apps/unified_search_app.py

# Verify environment
which python3  # Should show .venv/bin/python3
pip list | grep playwright
```

---

**END OF PERMANENT SECTION**
