Synthesize these research findings into a comprehensive structured report.

Original Question: {{ original_question }}

Research Summary:
- Tasks Executed: {{ tasks_executed }}
- Total Results: {{ total_results }}
- Entities Discovered: {{ entities_discovered }}

Entity Relationships:
{{ relationship_summary }}

Research Findings ({{ total_results }} results):
{{ top_findings_json }}

Coverage Summary:
{% if source_counts %}
{% for source, count in source_counts.items() %}
- {{ source }}: {{ count }} results
{% endfor %}
{% endif %}

Integrations Used: {{ integrations_used|join(', ') if integrations_used else 'None' }}

---

## TASK: Create Structured Synthesis Report

Return a JSON object with the following structure. **CRITICAL**: Every claim MUST have inline citations with URLs.

### Schema Requirements:

```json
{
  "report": {
    "title": "Research Report: [Title based on question]",

    "executive_summary": {
      "text": "3-5 sentences summarizing most important findings",
      "key_points": [
        {
          "point": "Key finding statement",
          "inline_citations": [
            {
              "title": "Source document title",
              "url": "https://...",
              "date": "YYYY-MM-DD or null",
              "source": "Integration name (e.g., USAJobs, SAM.gov)"
            }
          ]
        }
      ]
    },

    "source_groups": [
      {
        "group_name": "YOUR INTELLIGENT GROUP NAME",
        "group_description": "Why these sources are grouped together",
        "reliability_context": "Your assessment of this group's reliability and strengths",
        "findings": [
          {
            "claim": "Specific finding or claim",
            "inline_citations": [
              {
                "title": "Document title",
                "url": "https://...",
                "date": "YYYY-MM-DD or null",
                "source": "Integration name"
              }
            ],
            "supporting_detail": "Additional context, quote, or detail (optional, can be null)"
          }
        ]
      }
    ],

    "entity_network": {
      "description": "Narrative description of key entities and relationships",
      "key_entities": [
        {
          "name": "Entity name",
          "relationships": ["connected to X", "interfaces with Y"],
          "context": "Why this entity matters to the research question"
        }
      ]
    },

    "timeline": [
      {
        "date": "YYYY-MM-DD",
        "event": "Event description",
        "sources": [
          {
            "title": "Source title",
            "url": "https://..."
          }
        ]
      }
    ],

    "methodology": {
      "approach": "Brief description of search approach",
      "tasks_executed": {{ tasks_executed }},
      "total_results": {{ total_results }},
      "entities_discovered": {{ entities_discovered }},
      "integrations_used": {{ integrations_used|tojson if integrations_used else '[]' }},
      "coverage_summary": {{ source_counts|tojson if source_counts else '{}' }}
    },

    "synthesis_quality_check": {
      "all_claims_have_citations": true,
      "source_grouping_reasoning": "Brief explanation of your grouping strategy",
      "limitations_noted": "Any known limitations of this research (optional, can be null)"
    }
  }
}
```

---

## INSTRUCTIONS FOR SOURCE GROUPING (LLM Intelligence Required)

**DO NOT use hardcoded labels like [PRIMARY], [ANALYSIS], [LEAD].**

Instead, intelligently group sources based on:
1. **Authority level**: Official government sources vs industry analysis vs community discussions
2. **Source type**: Job boards vs news articles vs official documents vs social media
3. **Reliability**: Verified data vs aggregated analysis vs user experiences
4. **Relevance**: Direct answers vs contextual information vs background

**Example intelligent groupings**:
- "Official Government Publications" (USAJobs postings, SAM.gov contracts, DVIDS releases)
- "Industry Job Market Analysis" (ClearanceJobs aggregated data, market reports)
- "Community Insights and Experiences" (Reddit discussions, Discord conversations)
- "News and Media Coverage" (Brave Search news articles)

**Your task**: Analyze the sources in the findings and create meaningful groups with descriptive names.

---

## INSTRUCTIONS FOR INLINE CITATIONS (MANDATORY)

**Every claim MUST have at least one inline citation.**

Good examples:
- ✅ "Cleared cybersecurity roles require TS/SCI clearance for NSA positions" with citation to specific USAJobs posting
- ✅ "Contractor positions show $120K-$160K salary ranges" with citation to ClearanceJobs analysis
- ✅ "Reddit users report 6-month delays for polygraph clearances" with citation to specific Reddit thread

Bad examples:
- ❌ "Many cleared positions require TS/SCI" (no citation - which source?)
- ❌ "Salaries are competitive" (vague + no citation)

---

## READER TRUST & TRANSPARENCY

Your goal: Help readers assess the reliability of each claim.

Readers need to judge credibility themselves. Your job is to give them context:
- What type of source made this claim?
- How many independent sources confirm it?
- What's the source's potential bias or motivation?
- Is this firsthand reporting or secondhand?

---

## SOURCE VERIFICATION CONTEXT

Your goal: Help readers understand the evidentiary strength of each claim.

Readers need to distinguish between:
- What's officially confirmed vs reported by others
- What's directly observable vs inferred
- What's from primary sources vs secondhand accounts

When a claim comes from a leak, anonymous source, or is widely repeated:
Consider what level of verification exists and give readers context to assess it themselves.

---

## SYNTHESIS GUIDANCE

**Temporal Precision**:
- If research question mentions "2024", clarify if results cover FY2024 (Oct 2023-Sep 2024) or CY2024
- Use specific dates when available, not vague terms like "recent" or "lately"

**Terminology Interpretation**:
- If technical terms, acronyms, or jargon have multiple meanings, state which interpretation the results reflect
- Example: "RMF" could mean Risk Management Framework or other acronyms - clarify which

**Scope Clarity**:
- If results extend beyond or fall short of query scope, note this in limitations_noted
- Example: Query asked about "government contracts" but only DoD contracts were found

**Research Limitations Section**:
While writing the report, identify what information is MISSING that would strengthen the research:
1. **Missing Perspectives**: Whose voice/viewpoint is absent? (government officials, stakeholders, experts)
2. **Information Gaps**: What evidence/data would strengthen findings? (polling, official docs, statistics)
3. **Source Limitations**: What biases/limitations exist in the sources used? (e.g., "relied on English-language Western media")

Include these in the `synthesis_quality_check.limitations_noted` field.

**What NOT to Include in Final Report**:
- ❌ LLM decision-making process details (goes to execution_log.jsonl)

**Focus on**:
- ✅ Clear, verifiable claims with inline citations
- ✅ Source transparency (type, bias, confirmation)
- ✅ Intelligent source grouping with reliability context
- ✅ Entity relationships and connections
- ✅ Timeline of key events (if dates available)
- ✅ Research limitations (missing perspectives, gaps, source limitations)
- ✅ Clean, useful report for investigator

---

## QUALITY CHECKS BEFORE RETURNING

1. **Citation Check**: Does every claim in executive_summary.key_points and source_groups[].findings[] have at least one inline_citation?
2. **Source Grouping**: Did you create intelligent group names (not hardcoded labels)?
3. **Reliability Context**: Did you assess each group's reliability and strengths?
4. **Entity Context**: Did you explain why each key entity matters to the research question?
5. **Completeness**: Did you include all required fields from the schema?

Set `synthesis_quality_check.all_claims_have_citations` to `true` only if you verified every claim has citations.

---

Return ONLY the JSON object. No markdown, no explanations, just valid JSON matching the schema above.
