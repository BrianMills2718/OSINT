## 0. Overview

* Short description of what v1 does:

  * Takes a topic (e.g. *“psychological warfare and J-2”*).
  * Uses a deep-research engine to gather sources.
  * Extracts entities/claims into a simple knowledge graph.
  * Organizes everything into **Leads** (threads) you can grow over time.
  * Supports your style of **filter-based follow-up searches**.

---

## 1. Goals & Non-Goals (v1)

* **Goals**

  * Support *investigative journalism–style* deep dives, not just Q&A.
  * Keep **all raw text** (nothing thrown away).
  * Build a **persistent KG + leads** you can keep revisiting.
  * Reuse existing deep-research libraries (GPT-Researcher / Deep-Research / DeerFlow) as the engine.
* **Non-Goals**

  * No fancy NLI / truth pruning yet.
  * No enterprise auth / security.
  * No perfect ontology; “good enough” schema.

---

## 2. Core Data Model (Concepts)

### 2.1 Source

“Which document/page did this come from?”

* Fields (conceptually): `source_id`, `url`, `domain`, `search_engine`, `search_query`, `fetched_at`, etc.

### 2.2 Evidence

“Which exact text chunk did we see, from which source?”

* Fields: `evidence_id`, `source_id`, `snippet_text`, `location_in_source`, `fetched_at`, maybe `llm_notes`.

### 2.3 Entity

“People / orgs / projects / codewords / places / concepts.”

* Fields: `entity_id` (local), `label`, `type`, `aliases`, `optional_wikidata_qid`.

### 2.4 Claim (Triple)

“Our structured interpretation: subject–predicate–object.”

* Fields: `claim_id`, `subject_entity_id`, `predicate_id`, `object_entity_id_or_literal`, `evidence_id`, `extraction_model_version`.

### 2.5 Lead (Thread)

“Investigation line you care about.”

* Fields: `lead_id`, `title`, `hypothesis_text`, `status`, `seed_entities`, `seed_terms`, `attached_evidence_ids`, `open_questions`, `next_search_moves`.

### 2.6 SearchRun

“A single run of a deep-research job.”

* Fields: `search_run_id`, `lead_id`, `input_query_or_plan`, `engine_type` (gpt-researcher / deerflow / etc.), `parameters` (depth, breadth), `started_at`, `finished_at`, `cost_estimate`.

### 2.7 SearchPatternTemplate

“Reusable search strategy patterns (your boolean tricks).”

* Fields: `pattern_id`, `name`, `description`, `template_string` (with placeholders like `{PERSON}`, `{TOPIC}`), `used_for` (people×topic / office×capability etc.).

---

## 3. Storage Layout (Technically Simple)

* One relational database (e.g. Postgres or even SQLite for v1) with tables:

  * `sources`
  * `evidence`
  * `entities`
  * `claims`
  * `leads`
  * `search_runs`
  * `search_pattern_templates`
* Optional: simple `tags` table for tagging entities/leads (“high priority”, “psywar”, etc.).

---

## 4. External Components (Things You Don’t Build From Scratch)

### 4.1 Deep-Research Engine

* Pick **one** to start:

  * GPT-Researcher *or* Open Deep Research *or* DeerFlow.
* Your system calls it via API / CLI:

  * pass main question + maybe some hints.
  * get back: list of URLs, snippets, and a report.

### 4.2 Search / Scraping

* For v1: use what the engine already uses (Tavily, Firecrawl, Brave, etc.).
* Only enforce: **log** the queries and URLs it used so you can reconstruct.

### 4.3 LLM for Extraction & KG

* A separate LLM call (or small script) to:

  * extract entities from snippets,
  * convert them into claims,
  * decide which entities become seeds for new Leads.

---

## 5. Core Workflows

### 5.1 Start New Investigation (New Lead + Initial Deep Research)

* You type a topic (e.g. “psychological warfare and J-2”).
* System:

  * creates a new `Lead`;
  * launches one `SearchRun` using the deep-research engine;
  * ingests returned sources → evidence → entities → claims;
  * attaches best evidence/entities to the Lead.

### 5.2 Expand an Existing Lead (Pattern-Driven Follow-up)

* You open a Lead and choose:

  * “Expand using pattern: Person×Topic” or “Office×Capability” etc.
* System:

  * pulls related entities from KG (e.g. J-2 heads);
  * instantiates search templates into **concrete queries**;
  * for each query: runs another `SearchRun`, ingests results, updates KG and Lead.

### 5.3 Prospector Pass (Suggest New Leads)

* System periodically scans KG + evidence:

  * finds unusual co-occurrences (e.g. person repeatedly appearing with J-2 + psyops),
  * rare but recurring codewords,
  * bridge entities linking multiple themes.
* Suggests new Leads:

  * “Candidate Lead: Jane R. Doe as cross-program actor. Accept?”

### 5.4 Browse / Query Layer for You

* Very simple v1 UI / CLI actions:

  * list all Leads, filter by tag / status.
  * open a Lead → see:

    * hypothesis, open questions, attached evidence, entities, search runs.
  * simple queries over KG:

    * “show all claims involving J-2 and psychological warfare”
    * “list entities co-occurring with Operation FORESIGHT”.

---

## 6. Search Pattern System (Your Investigator Tricks)

* A small set of **template types**:

  * Person×Topic
  * Office×Capability
  * Codename×Org
  * Time-bounded variants
* For each:

  * define template string,
  * rules for filling placeholders from KG (which entities/terms),
  * how many concrete queries to generate per Lead.

---

## 7. Phase 2 / Later (Not in Minimal v1)

* NLI / entailment filters to down-rank conflicting / low-support claims.
* More ontology / TBox (richer types, subproperties).
* Multiple deep-research engines with routing.
* Time-aware graph views (“state of world as of 2012”).
* Better UI (graph visualization, timelines).

---


### 1. Goals & Non-Goals (v1)

#### 1.1 Primary goals

1. **Be a power tool for *you*, not a general assistant**

   * Optimized for **investigative journalism workflows**:

     * Following threads (people, offices, programs, codewords, documents)
     * Pivoting: “now find *all* J-2s since X and see where else they show up”
     * Building long-running investigations over weeks/months

2. **Stand on the shoulders of existing deep-research libraries**

   * Use something like **GPT-Researcher / Deep Research / DeerFlow** as:

     * Planner (sub-questions)
     * Search + scraping engine
     * First-pass summarizer/report generator
   * Your system **wraps** these instead of re-implementing them.

3. **Treat everything as research material, not “truth”**

   * The core job is:

     > *Capture what was claimed where and when, plus how it connects.*
   * v1 does **not** try to decide what’s true.
   * It **preserves text** and **captures structure** (entities, relationships, leads).

4. **Keep a permanent, queryable memory of your work**

   * Every research run:

     * Saves **sources** (URLs, domains, search queries, timestamps)
     * Saves **evidence** (snippets/paragraphs)
     * Extracts **entities** & **claims** into a small KG
   * You can later ask:

     * “Where have I seen `Operation XYZ` before?”
     * “Show me all snippets linking `J-2` and `psychological warfare`.”

5. **Support sophisticated search strategies**

   * Encode “expert moves” as **search patterns**, for example:

     * `For each entity with role = "J-2", search: "{name}" AND "psychological operations"`
     * `For each program related to "psychological warfare", search: "{program} AND budget OR contract OR RFP"`
   * System:

     * Pulls entities from KG
     * Fills in patterns
     * Runs deep research
     * Ingests new evidence/claims into the same Lead

6. **Be small, understandable, and evolvable**

   * One small service + one DB (Postgres) is enough.
   * Clear tables, clear flows; no Kafka, no streaming, no NLI in v1.
   * Easy to extend later (more entity types, better extraction, ranking, “truthiness”, etc.).

---

#### 1.2 Non-goals (for v1)

1. **No “truth adjudication” or fact-checking engine**

   * v1 does **not**:

     * Decide which claims are correct.
     * Do multi-source logical consistency checks.
   * It might tag “same claim seen in N sources,” but that’s just **signal**, not a verdict.

2. **No real-time / event streaming infrastructure**

   * No Kafka, Debezium, CDC, or “keep the graph within seconds of reality”.
   * Research runs are **batchy**:

     * You trigger them manually (or via simple jobs in v2+).

3. **No complex ontology / formal logic (OWL, description logic, etc.)**

   * v1 has:

     * A simple notion of **entity types** (person, org, project, concept, office, etc.)
     * A small set of **relation labels** (held_role, part_of, related_to, mentioned_in, etc.)
   * No need for:

     * Reasoners
     * TBox/ABox separation in a formal logic sense
     * Full OWL expressivity

4. **No multi-tenant, enterprise-grade security / RBAC**

   * Assume:

     * It’s a **personal tool** (maybe a small trusted team later).
   * Basic auth / API keys / local network is enough in v1.

5. **No fully custom deep-research engine (yet)**

   * You **do not**:

     * Implement your own multi-agent system from scratch.
     * Build your own ReAct/ToT orchestration layer.
   * You **wrap** one of the existing frameworks as a “black box” for:

     * `plan + search + scrape + summarize`.

6. **No polished product UI or public SaaS**

   * A simple (even ugly) UI is acceptable:

     * List of leads
     * Lead detail view (entities, evidence, claims)
     * A couple of search pattern buttons
   * Optimization is for **power**, not for onboarding strangers off the internet.

---

#### 1.3 Design north star

> **You + v1 should be able to do a serious, months-long investigation, and when you come back to it later, you can see exactly:**
>
> * What was read
> * What was claimed
> * Which entities/relationships emerged
> * What search maneuvers were used to dig deeper

Everything else (truth scores, NLI, fancy streaming, ontology, etc.) is v2+.




### 2. Core mental model: Leads, runs, and the graph

#### 2.1 Lead = your “case file”

A **Lead** is the top-level unit of work. One lead = one investigation thread.

Examples:

* Lead A: `“Psychological warfare programs involving J-2 since 2001”`
* Lead B: `“Use of private contractors in signals intelligence”`

A Lead holds:

* The **high-level question**
* All **research runs** you did for it
* All **sources, evidence, entities, and claims** discovered under that umbrella

Think of it as a manila folder with everything related to that investigation.

---

#### 2.2 Research Run = one deep-research operation

A **Research Run** is:

> “I asked the deep-research engine X, with this query + settings, and it produced this report + these sources.”

Each run has:

* `lead_id` – which Lead it belongs to
* `query` – what you asked (e.g., *"history of J-2 involvement in psychological operations"*)
* `engine` – which framework you used (`gpt-researcher`, `deep-research`, `deer-flow`, etc.)
* `config` – breadth/depth, models, etc.
* `raw_report` – the markdown/HTML report it produced
* `status / timestamps`

Then v1 does **post-processing** on the run output:

* Parses the report + citations
* Creates or updates:

  * **Sources** (URLs, search queries, domains)
  * **Evidence** (snippets from those sources)
  * **Entities** (people, orgs, offices, programs, concepts, codewords)
  * **Claims** (structured relationships between entities)

So: **Run = “call out to an existing deep research library; then ingest what it found.”**

---

#### 2.3 Sources and Evidence = “what text we saw where”

Within each Research Run:

* A **Source** = “this URL (or document / transcript) we touched”

  * `source_id`, `url`, `domain`, `search_query_used`, `first_seen_at`, etc.

* An **Evidence** row = “this specific snippet / paragraph from that source”

  * `evidence_id`, `source_id`, `snippet_text`, `location_in_source`, `fetched_at`

You can later ask:

* “Show me all **evidence snippets** that mention `J-2` and `psychological warfare`”
* “Which **sources** did I rely on heavily in this lead?”

---

#### 2.4 Entities = the “things that matter” to you

From reports + evidence, v1 extracts **Entities**:

* People: `J-2 John Smith`, `Col. Jane Doe`
* Orgs: `Joint Staff J-2`, `CIA`, `JSOC`
* Programs: `Operation XYZ`, `Project ABC`
* Concepts: `psychological warfare`, `information operations`
* Offices/Roles: `Deputy Director for Intelligence`, `J-2`, `Undersecretary for X`

Each entity:

* Has a **local ID**: `E123`
* Has labels / aliases: `"J-2"`, `"Joint Staff J-2"`, `"Directorate for Intelligence"`
* Has a coarse **type**: `person`, `org`, `office`, `program`, `concept`, etc.
* May optionally have:

  * `wikidata_qid` if known (for later mapping)
  * URLs / notes

Over time, as you run more research under the same Lead (or across Leads), you build a **reusable roster** of entities you care about.

---

#### 2.5 Claims = structured connections you can pivot on

A **Claim** is a structured statement derived from some evidence:

* `subject_entity_id` – `E_person_JaneDoe`
* `predicate` – e.g. `held_role`, `part_of`, `related_to`, `mentioned_in_context_of`
* `object_entity_id` or `literal` – another entity or a value
* `evidence_id` – which snippet this came from
* `run_id`, `lead_id`
* Optional: `confidence`, `notes`

Examples:

* `Jane Doe` **held_role** `Director of J-2`
* `J-2` **responsible_for** `psychological operations planning`
* `Operation XYZ` **mentioned_with** `psychological warfare`
* `J-2` **mentioned_in_source** `[some URL]` (weak, but useful for recall)

This is your **knowledge graph**:

> “Here’s what has been *claimed* in the material I’ve processed.”

You are not saying it is *true*, just that it was extracted and anchored to specific text.

---

#### 2.6 How this matches your investigative workflow

For a topic like **psychological warfare**:

1. You create a **Lead**:
   `“Psychological warfare programs involving J-2 since 2001”`.

2. You run a **Research Run** using `gpt-researcher` or `deer-flow`:

   * It plans sub-questions, searches, scrapes, summarizes.

3. v1 ingests the output:

   * Adds **Sources** (DoD PDFs, news articles, think-tank reports)
   * Adds **Evidence** snippets (paragraphs mentioning J-2, psyops, program names)
   * Extracts **Entities**:

     * `J-2`, names of people holding J-2, program names, codewords
   * Extracts **Claims**:

     * `J-2` linked to `psychological operations`
     * Person X held `J-2` between years Y–Z
     * Program Q is `part_of` some office

4. Now you can do “expert maneuvers” using the graph:

   * “List all **people** who held a `J-2`-type role.”
   * For each, run a **new Research Run**:

     * Query pattern: `"{person_name}" AND ("psychological operations" OR "PSYOP" OR "information operations")`
   * Ingest those runs back into the same Lead.

5. Over time, the Lead’s graph becomes:

   * A dense map of **who**, **where**, and **how** psywar is discussed
   * All anchored to **actual text** you can click back to

That’s the mental model:

* **Lead** = case file
* **Runs** = you sending out a “research squad” and filing their report
* **Sources/Evidence** = the raw material
* **Entities/Claims** = the structured map you pivot and search on



### 3. Minimal data model for v1

#### 3.1 Leads

```text
Lead
- id
- title              # "Psychological warfare & J-2 since 2001"
- description        # optional notes
- created_at
- updated_at
```

1 row per “case file”.

---

#### 3.2 Research Runs

```text
ResearchRun
- id
- lead_id            # FK → Lead
- engine             # "gpt-researcher" | "deep-research" | "deer-flow"
- query_text         # what you asked that engine
- config_json        # breadth/depth, model, etc.
- started_at
- finished_at
- raw_report_md      # the big markdown report it produced
- status             # "success" | "failed" | "partial"
```

This is the bridge between “call deep research library” and “ingest into KG”.

---

#### 3.3 Sources (documents, pages)

```text
Source
- id
- run_id             # FK → ResearchRun (first run that saw it)
- lead_id            # denormalized for convenience
- url                # or file path / identifier
- domain             # parsed from URL
- title              # best-effort page title
- content_type       # "html" | "pdf" | "news" | "report" | "tweet" ...
- search_query_used  # e.g. "J-2 psychological operations 2003"
- first_seen_at
- last_seen_at
```

1 Source ≈ 1 document / page / item.

---

#### 3.4 Evidence (snippets of text)

```text
Evidence
- id
- source_id          # FK → Source
- run_id             # FK → ResearchRun
- lead_id
- snippet_text       # the paragraph / span
- location_hint      # "p42", "line 330-360", xpath, etc.
- fetched_at
- extraction_method  # "auto_snippet" | "llm_selected" | "manual"
```

You **always** keep the snippet text here. This is your “what we actually saw” anchor.

---

#### 3.5 Entities (people, orgs, programs, concepts…)

```text
Entity
- id
- lead_id            # entities are scoped to a lead in v1
- canonical_name     # "Joint Staff J-2"
- entity_type        # "person" | "org" | "office" | "program" | "concept" | ...
- description        # optional human-written gloss
- wikidata_qid       # nullable; fill in when easy (Q11214, etc.)
- created_at
- updated_at
```

Aliases:

```text
EntityAlias
- id
- entity_id
- alias_text         # "J-2", "Directorate for Intelligence"
- source_of_alias    # "llm_extracted" | "from_evidence" | "manual"
```

This lets search be robust to different spellings / phrasing.

---

#### 3.6 Claims (structured relations anchored to evidence)

```text
Claim
- id
- lead_id
- run_id
- evidence_id        # FK → Evidence (NULL allowed for future/manual claims)
- subject_entity_id  # FK → Entity
- predicate          # string ID; e.g. "held_role", "part_of", "mentioned_with"
- object_entity_id   # FK → Entity, nullable
- object_literal     # text or simple value if no entity (e.g. a year)
- extraction_method  # "llm_heuristic_v1" | "manual"
- confidence         # simple 0–1 or 1–5 scale (optional)
- created_at
```

Predicates (simple v1):

```text
Predicate
- id                 # "held_role", "part_of", "related_to", "mentioned_with"
- description
- suggested_domain   # optional: what types subject/object should be
- wikidata_pid       # optional mapping; e.g. P39 for "position held"
```

You don’t need to overdo ontology here; a small hand-curated list is enough for v1.

---

#### 3.7 Saved searches / “investigator tactics” (optional but cheap)

To support reusable “expert moves” (like your “name AND codeword” patterns), a light table:

```text
SearchPattern
- id
- lead_id           # or NULL for global
- label             # "Person AND psywar terms"
- template          # e.g. "{{person_name}} AND (\"psychological operations\" OR \"PSYOP\" OR \"information operations\")"
- notes
```

Later your UI can:

* Let you pick a person entity → apply a `SearchPattern` → spin up a new ResearchRun with the rendered query.

---

#### 3.8 How it all connects (one concrete row-flow)

Example for a single paragraph:

1. `ResearchRun 17` (query: *"J-2 involvement in psychological warfare"*) finishes.
2. It hit `https://example.mil/report.pdf`:

   * Create `Source 200` (url, domain, etc.).
3. From that PDF, you select a paragraph:

   > “In 2005, the Joint Staff J-2 oversaw the development of Operation NIGHTFIRE, a psychological operations initiative…”

   * Create `Evidence 900` with that text, location, and link it to `Source 200` and `Run 17`.
4. You extract entities:

   * `Entity 10`: “Joint Staff J-2” (type: office)
   * `Entity 11`: “Operation NIGHTFIRE” (type: program)
   * `EntityAlias` for “J-2”, “NIGHTFIRE”, etc.
5. You extract claims:

   * `Claim 5001`:

     * subject_entity_id = 10 (“Joint Staff J-2”)
     * predicate = "oversaw" (or "responsible_for")
     * object_entity_id = 11 (“Operation NIGHTFIRE”)
     * evidence_id = 900
     * run_id = 17

Now later you can:

* “Show all **programs** where some **office** has predicate `oversaw` AND some evidence snippet mentions ‘psychological operations’.”
* “For each `J-2` holder I’ve found, run a pattern search and ingest new Sources/Evidence/Claims.”



### 4. Ingestion & annotation pipeline (v1)

#### 4.1 From user action to ResearchRun

Flow:

1. You pick a **Lead** and type a query:

   > “J-2 involvement in psychological warfare since 2001”

2. System creates a `ResearchRun` row (status = `running`).

3. System calls **one deep-research engine** (pick one for v1, e.g. GPT Researcher or DeerFlow) with:

   * `query_text`
   * config (breadth/depth, model, etc.)

4. When the run finishes, we store:

   * `raw_report_md` = full markdown report
   * `status` = `success` (or `failed`)

This gives you a stable record: “this query, with this config, produced this report at this time.”

---

#### 4.2 Extract Sources from the report

From `raw_report_md` we:

1. **Parse citations / links**:

   * Every URL referenced in the report → candidate `Source`.
   * For each URL:

     * If not already in `Source` for this lead:

       ```text
       Source
       - id
       - run_id = this ResearchRun
       - lead_id
       - url
       - domain
       - title            # from HTML <title> if we can fetch
       - content_type     # rough guess
       - search_query_used = query_text (or the search sub-query if we have it)
       - first_seen_at, last_seen_at
       ```

2. **Optional v1 shortcut**:
   For the very first version, you can skip fetching full HTML/PDF and rely only on the snippets that the deep-research engine already quoted.
   If you *do* fetch, store the raw content somewhere (file/object store) keyed by `source_id`. You don’t have to normalize it yet.

This step converts “URLs in a narrative report” into structured `Source` rows.

---

#### 4.3 Extract Evidence snippets

For each **citation + quote** in the report:

* Example report text:

  > According to a 2005 Joint Staff report,[^1] the J-2 oversaw the development of Operation NIGHTFIRE, a psychological operations initiative…

  And a footnote:

  > [^1]: [https://example.mil/psyop-2005.pdf](https://example.mil/psyop-2005.pdf)

We:

1. Identify the **snippet** being attributed to that source:

   * The sentence or paragraph around the citation.

2. Create an `Evidence` row:

   ```text
   Evidence
   - id
   - source_id        # link to the Source for https://example.mil/psyop-2005.pdf
   - run_id
   - lead_id
   - snippet_text     # that paragraph or sentence
   - location_hint    # can be "from_report_section:2" in v1 if we don't know the exact page
   - fetched_at       # now
   - extraction_method = "from_report_citation"
   ```

3. **Optional enrichment** (if you fetched the full page/PDF):

   * Later, a background job can map `location_hint` to a PDF page / HTML fragment and refine it.

So at this point, without doing any extra crawling, you already have:

* All the URLs the research agent used (`Source`)
* All the text it quoted as evidence (`Evidence`)

---

#### 4.4 Entity extraction from Evidence

For each `Evidence` row:

1. Run an **LLM extraction prompt**:

   Input:

   * `snippet_text`

   Output (JSON):

   ```json
   {
     "entities": [
       {"name": "Joint Staff J-2", "type": "office"},
       {"name": "Operation NIGHTFIRE", "type": "program"},
       {"name": "psychological operations", "type": "concept"},
       {"name": "2005", "type": "year"}
     ]
   }
   ```

2. For each extracted entity:

   * Try to **match** to existing `Entity` for this lead:

     * Exact match on `canonical_name`
     * Or any `EntityAlias.alias_text`

   * If no match → create new `Entity`:

     ```text
     Entity
     - canonical_name = "Joint Staff J-2"
     - entity_type    = "office"
     - lead_id
     ```

   * Create `EntityAlias` if the surface form differs:

     ```text
     EntityAlias
     - entity_id       # that Entity
     - alias_text      # "J-2"
     - source_of_alias = "llm_extracted"
     ```

This is how your “universe of things” grows automatically as you ingest more evidence.

---

#### 4.5 Claim extraction from Evidence

Still per `Evidence` row, with the entities we just attached:

1. Run another **LLM prompt** to propose **Claims**:

   Input:

   * `snippet_text`
   * entities (with IDs + types)
   * allowed predicates list (small in v1, e.g.):

     * `held_role`
     * `oversaw`
     * `part_of`
     * `located_in`
     * `mentioned_with` (very loose, for co-occurrence)
     * `related_to`

   Output:

   ```json
   {
     "claims": [
       {
         "subject": "Joint Staff J-2",
         "predicate": "oversaw",
         "object": "Operation NIGHTFIRE",
         "confidence": 0.78
       }
     ]
   }
   ```

2. For each claim:

   * Map `subject` / `object` names → `Entity` IDs (from previous step).
   * Create a `Claim` row:

     ```text
     Claim
     - lead_id
     - run_id
     - evidence_id
     - subject_entity_id   # Entity("Joint Staff J-2")
     - predicate           # "oversaw"
     - object_entity_id    # Entity("Operation NIGHTFIRE")
     - object_literal      # NULL in this example
     - extraction_method   # "llm_heuristic_v1"
     - confidence          # 0.78
     ```

3. If the object is not an entity (e.g. a year or a number):

   ```text
   Claim
   - subject_entity_id = Entity("Operation NIGHTFIRE")
   - predicate         = "started_in_year"
   - object_entity_id  = NULL
   - object_literal    = "2005"
   ```

Now every structured statement in your graph is *anchored* to a specific `Evidence` snippet and ultimately to a `Source` URL.

---

#### 4.6 Minimal human-in-the-loop (optional v1)

Even in v1 you can have a very light review step, but keep it optional:

* A UI that shows:

  > Snippet: “In 2005, the Joint Staff J-2 oversaw the development of Operation NIGHTFIRE, a psychological operations initiative…”

  Proposed entities:

  * Joint Staff J-2 (office)
  * Operation NIGHTFIRE (program)

  Proposed claim:

  * Joint Staff J-2 — **oversaw** → Operation NIGHTFIRE

* You can:

  * Accept / reject the claim
  * Fix the entity type (“program” vs “operation”)
  * Merge duplicate entities

Even if you skip this at first, the pipeline is set up so you can add review later without changing the core tables.



### 5. Query & exploration UX (v1)

#### 5.1 Main entry points

You don’t need a fancy UI in v1, but you *do* need a few solid starting points:

1. **Lead page**

   For each Lead, you see:

   * Recent `ResearchRun`s (title, time, status)
   * Top entities (by how often they appear in Evidence)
   * Recent claims (table: subject – predicate – object – source domain – date)

   So a typical lead page:

   * “J-2 & psychological warfare”
   * Runs:

     * 2025-11-10 – “J-2 psyops overview” – success
     * 2025-11-12 – “Operation NIGHTFIRE details” – success
   * Entities:

     * Joint Staff J-2 (office, 19 mentions)
     * Operation NIGHTFIRE (program, 11 mentions)
     * US Special Operations Command (org, 6 mentions)
   * Claims (latest 20)

2. **Entity page**

   For each Entity:

   * Basic info: name, type, aliases, optional description
   * “Mentioned in” = list of Evidence snippets
   * “Connected to” = graph-ish list of Claims

   Example: entity “Joint Staff J-2”

   * Mentions:

     * “…the J-2 oversaw the development of Operation NIGHTFIRE…”
     * “…according to multiple Pentagon officials in J-2…”
   * Claims:

     * J-2 — oversaw → Operation NIGHTFIRE
     * J-2 — part_of → Joint Staff

3. **Global search bar**

   One box, 2 modes:

   * **Entity search**: type “NIGHTFIRE” → list entities & aliases matching that string.
   * **Full-text search** (over Evidence.snippet_text) for “NIGHTFIRE psyop 2005”.

   v1 can be “simple LIKE search” in Postgres; you don’t need dense embeddings yet.

---

#### 5.2 Evidence-centric browsing

Because your deep work lives in snippets, give Evidence a first-class view.

**Evidence table view** (filterable list):

Columns:

* Date (fetched_at)
* Snippet (truncated)
* Source (domain + icon)
* Linked entities (chips)
* Run (which ResearchRun it came from)

Filters:

* By **entity**:

  * Show all Evidence where `entity_id = J-2` appears (subject, object, or mentioned).
* By **domain**:

  * Only `.mil`, `.gov`, or specific outlets.
* By **time**:

  * Evidence fetched in the last N days / restricted to documents mentioning specific years.
* By **predicate** (via Claims join):

  * Only Evidence that supports claims with predicate `oversaw` or `supplied`.

This gives you a “fact table” you can slice like an investigator: “show me all snippets where J-2 appears in .mil domains between 2001–2010”.

---

#### 5.3 Graph / claim exploration

You don’t need a fancy node-link graph in v1; a **claims table** + simple pivoting goes a long way.

**Claims table view:**

Columns:

* Subject (Entity)
* Predicate (string)
* Object (Entity or literal)
* Evidence (link)
* Source (domain)
* Confidence
* Run

Filters:

* Subject = “Joint Staff J-2”
* Predicate in [oversaw, held_role, part_of]
* Object type = program / operation / organization

Example usage:

* Filter: subject = J-2, predicate = oversaw

  * See all programs / operations J-2 is claimed to oversee.
* Filter: predicate = held_role, object = “Director of J-2”

  * See all people who held that office, over what Evidence.

Even with this simple table, you can answer lots of graph-like questions.

---

#### 5.4 “Investigator-style” workflows

Concrete examples in your terms:

1. **“Find all J-2 who touched psychological warfare”**

   Steps:

   1. Go to entity “psychological operations” (concept).
   2. See connected entities / claims:

      * Operation NIGHTFIRE — has_topic → psychological operations
      * Operation SHADOWCAST — has_topic → psychological operations
   3. Click Operation NIGHTFIRE:

      * Claims:

        * J-2 — oversaw → Operation NIGHTFIRE
        * Person X — held_role → Director of J-2
   4. Now pivot:

      * “Show me all people with held_role where office = J-2”.
      * You now have a list of J-2 office holders (Entities of type person).

   This is how you replicate “oh J-2 runs X → who were the J-2s?”.

2. **“Boolean-style narrowing with KG + web search”**

   You mentally do:

   > (J-2) AND (Operation NIGHTFIRE) AND (psyop OR psychological operations)

   v1 version:

   * Start from entity “Operation NIGHTFIRE”
   * Click “Search web with bundle” → system builds a query:

     * `"Operation NIGHTFIRE" "J-2" psychological operations site:.mil`
   * Sends that to your deep-research engine as a new `ResearchRun`, tagged:

     * input_entities = [Operation NIGHTFIRE, J-2, psychological operations]

   When results come back, new Sources/Evidence/Claims are attached to those entities.
   Over time, this “search with KG bundle” becomes your main power move.

---

#### 5.5 Saved filters & patterns (minimal)

v1 doesn’t need full query-language; you can get far with **saved views**:

* Example saved view: “All Evidence where:

  * entity = J-2
  * domain IN (.mil, .gov)
  * snippet_text ILIKE ‘psyop%’ OR ‘psychological operations’”

Store that as:

```text
SavedView
- id
- lead_id
- name = "J-2 psyop .mil/.gov"
- definition_json = {
    "entities": [E_J2],
    "domains": [".mil", ".gov"],
    "text_filters": ["psyop", "psychological operations"]
  }
```

Then from the UI you can:

* Click the saved view → instantly see new Evidence matching those filters as it comes in.
* Use it as a **feed**: “anything new here is potentially interesting”.

---

#### 5.6 How deep-research runs plug in

From the UX perspective, a `ResearchRun` is:

* A way to **expand** the universe
* Seeded either by:

  * Free-text question (“What psychological operations are attributed to J-2 since 2001?”)
  * KG-aware search (“Search web for Operation NIGHTFIRE + J-2”)

In v1:

* You pick:

  * Lead
  * Query text
  * (Optional) a few entities to “pin” into the search
* System:

  * Starts `ResearchRun`
  * Calls one engine (GPT Researcher / Deep Research / DeerFlow)
  * Runs your ingestion pipeline (Sources → Evidence → Entities → Claims)
* Then:

  * New entities appear in your entity lists
  * New claims appear in Claims table
  * Saved views automatically show fresh Evidence

So the “deep research aspect” is operationalized as:

* **Plan & search** lives inside the engine (GPT Researcher / DeerFlow).
* **Memory, filtering, and investigative method** lives in:

  * Evidence table
  * Entity pages
  * Claims table
  * Saved views + KG-aware search buttons.

This is enough in v1 to start behaving like a human investigator:

* spot interesting entities,
* pivot around them,
* and launch new deep runs that deliberately combine them.



### 6. How this actually mimics an expert investigator

Think of v1 as “a brain with good habits” rather than just “run an agent and dump a PDF”.

I’ll walk it in the same moves you’d do manually.

---

#### 6.1 Start broad, then aggressively narrow

You: “psychological warfare J-2 history and programs since 2001”.

1. **Create a Lead**

   * Lead: `L1 = "J-2 & psychological warfare since 2001"`

2. **Run a deep-research job for that lead**

   * `ResearchRun #1`:

     * query: that sentence
     * breadth/depth tuned moderately high

3. **Engine does its thing (inside GPT Researcher / DeerFlow)**

   * Plans subquestions:

     * “What is J-2?”
     * “Known J-2 psyops programs?”
     * “Key doctrinal docs on US psyops post-2001?”
   * Hits Brave / Tavily / Firecrawl, scrapes, summarizes.
   * Returns:

     * a long markdown report
     * list of URLs and snippets (Sources/Evidence)

4. **You don’t just read the report; you mine it**

From that run, the pipeline gives you:

* Entities:

  * Joint Staff J-2 (office)
  * Various named operations / programs
  * People (directors, commanders, spokespeople)
  * Concepts (“psychological operations”, “information operations”)
* Claims:

  * J-2 — oversaw → Operation NIGHTFIRE
  * Operation NIGHTFIRE — has_topic → psychological operations
  * Person X — held_role → Director of J-2
* Evidence:

  * concrete snippets with URLs / dates.

Your Lead page now looks like “broad first pass done; here are the interesting nouns and relationships”.

---

#### 6.2 Turn “interesting nouns” into new investigative angles

You skim entity lists / claims and form hypotheses like a human:

* “This Operation NIGHTFIRE keeps popping up, but always vaguely.”
* “Person X seems to show up in multiple psyops contexts.”

You then:

1. Open entity “Operation NIGHTFIRE”:

   * See all evidence snippets mentioning it.
   * See connected entities:

     * J-2
     * US SOCOM
     * maybe foreign offices, contractors.

2. From that page, launch **targeted deep runs**:

   * “Search web for Operation NIGHTFIRE + J-2 + 'psychological operations' site:.mil”
   * “Search web for Operation NIGHTFIRE + contractor names”

Implementation-wise:

* You click “Run deep research on this entity”
* UI proposes a few query templates:

  * `"Operation NIGHTFIRE" "J-2" psychological operations site:.mil`
  * `"Operation NIGHTFIRE" "psyop" site:.gov OR site:.mil`
* Each becomes a new `ResearchRun`, tagged with:

  * input_entities = [Operation NIGHTFIRE, J-2, psychological operations]

When those runs finish, you don’t just get more blobs of text; you get more Entities, Claims, Evidence, all attached back to the same Lead and entities.

---

#### 6.3 Boolean-style narrowing, but backed by the KG

Your mental move:

> “Find all J-2 holders in last 20 years, then AND with psyops stuff.”

In v1:

1. Use **Claims table**:

   * Filter: predicate = held_role, object = “Director of J-2”
   * Result: list of people + evidence.

2. Turn that into a **saved pattern**:

   “For each person who was Director of J-2 between 2001–2025, run deep research combining:

   * their name
   * psyop / psychological operations / information operations
   * site:mil, site:gov, plus a couple of relevant outlets”

V1 version of that:

* You manually iterate (one per person) or semi-automate (simple loop later).
* Each person gets several `ResearchRun`s with the KG entities baked into queries.

So the KG gives you the **sets** (all J-2 directors), and the deep-research engine gives you **content** about those sets.

---

#### 6.4 Building up pattern-of-life / pattern-of-activity

Over time, for a Lead:

* Entity “Person X” accumulates:

  * Roles (claims like Person X — held_role → Director of J-2)
  * Programs/projects (Person X — associated_with → Operation Y)
  * Quotes/interviews (Evidence from news, reports, transcripts)
  * Co-mentions with organizations or other people.

You can:

* Go to Person X entity page and see:

  * Timeline-ish view (Evidence sorted by date)
  * Graph-ish view (who/what they connect to)
* Filter Evidence:

  * entity = Person X
  * domain in [.mil, main outlets you trust]
  * snippet_text contains ["psyop", "IO", "perception management"]

That’s exactly the “pattern-of-life” thing human investigators build, but now:

* Underpinned by structured Claims and Entities.
* Always traceable back to raw Evidence / Sources.

---

#### 6.5 Iterative narrowing loop (how v1 supports “deep”)

The deep-ness isn’t just “depth = 3” in the search agent; it’s the loop:

1. **Broad run on a Lead** → discover entities/claims.
2. **You pick promising entities / patterns**

   * operations, offices, people, codewords, odd phrases.
3. **Targeted deep runs** seeded with:

   * natural-language prompt
   * plus pinned entities from the KG.
4. **Pipeline ingests** results again → KG gets denser.
5. **Saved views** and filters surface:

   * new evidence that fits your investigative patterns.

Repeat until you:

* See convergence (the same facts/domains repeating)
* Or discover new surprising entities/claims that merit another loop.

V1 doesn’t automate the *strategy* (you’re still the editor), but it:

* Remembers what you’ve already seen.
* Lets you pivot quickly via entities/claims.
* Makes “boolean-style narrowing” a first-class interaction:

  * Instead of raw `(term1 AND term2 AND term3)`, you’re doing:

    * “all evidence where entity A and concept B and domain C and year D… and go launch more research from there.”

That’s the core of “human-level investigative methodology” without over-building the first version.




###UNCERTANITES AND UNKOWNS
. Methodology risk: does it actually feel like investigative research?
Uncertainty

Deep-research libraries (GPT Researcher, DeerFlow, etc.) are mostly optimized for:

“Give me a good report”

“Summarize many sources nicely”

They’re not explicitly optimized for:

Surfacing weird outliers

Preserving fragile leads

Building “search strategies” (your J-2 AND codeword style work)

Risks

They may over-summarize the interesting weird bits.
The LLM tries to be coherent and “balanced,” which can wash out the odd, disturbing, or low-frequency clues that are gold for you.

They may not respect your search tactics.
If you say “look at J-2 AND psychological operations AND site:.mil”, the framework might:

turn it into something softer (“find info about J-2 and psyops”)

ignore the AND / site filters you care about.

They are “report-first”, not “lead-first”.
You care about lead generation and search-space narrowing at least as much as “a nice write-up”.

Mitigations (v1-level)

Always store all evidence and expose:

a raw snippet browser per lead

full-text search over evidence.snippet_text
So if the report doesn’t mention something, you can still find it.

Add a second “exploration mode” prompt for the LLM that explicitly says:

“List unusual entities, codewords, units, offices, and phrases that look like potential investigative leads. Don’t collapse them into a narrative; just list them.”

For critical boolean-ish tactics, don’t rely on the deep-research engine:

Build a very simple:

build_query_from_entities_and_keywords(...)

call Brave search directly

save those results as a separate ResearchRun
Use the engine mainly to read & summarize what your targeted search surface found.

2. Extraction risk: entities & claims will be messy
Uncertainty

We’re asking an LLM to:

recognize entities

map them to your existing list

express relationships as claims

This is powerful but imperfect.

Risks

Entity conflation

Different people/orgs with similar names get merged.

“J-2” might refer to slightly different offices in different contexts.

Over-asserted structure
The LLM may create a clean claim like:

J-2 -> runs_program -> X

…even if the text only said “J-2 was briefed on X”.

Schema drift
As you add predicates (“reported_on”, “involved_in”, “rumored_connection_with”), your predicate list may sprawl and become inconsistent.

Mitigations

Design for ambiguity, not correctness:

claims = “this is one plausible reading of that snippet”

you stay in charge of what you trust

Expose “show me the snippet” everywhere:

Every claim UI element should have a 1-click link to the underlying evidence text + source.

Support text search alongside KG search:

If you suspect entity conflation, you can always:

search evidence.snippet_text for the raw string (“J-2”, “J-2, Intelligence Directorate”)

bypass the entity layer entirely.

Keep predicates small and curated in v1:

Start with a handful:

held_role, part_of, reported_on, mentioned_with, associated_with

Add more only when you really feel the gap.

3. Coverage & bias risk: what you don’t see
Uncertainty

Your system’s worldview will depend heavily on:

which search API hits (Brave / Tavily / Firecrawl)

which countries / languages it’s good at

which sites block bots

Risks

Search API bias

Brave / Tavily may under-represent certain languages, regions, or fringe sites.

Their ranking algorithm is opaque.

Robots / anti-bot

Some important sources might not be reachable via automated scraping at all.

Silent gaps
The system will rarely say:

“I failed to search in this direction.”
It just shows a report, which can make the space look more complete than it is.

Mitigations

Always log and display what search_queries were actually used and how many sources were found.

Per ResearchRun, keep:

list of queries

counts of results per query

which domains showed up.

Give yourself a “manual seed URLs” ability:

Let the lead UI accept:

“Also crawl these 3 specific URLs”
So when your human nose says “this forum looks important,” you can force it in.

Flag thin runs:

If a run yields < N sources, mark it as “low coverage”.

That reminds you to not overtrust it.

4. Cost, latency, and scale risk
Uncertainty

Deep research workflows:

call the LLM many times

call search many times

may explode in cost if you run them aggressively

Risks

Bill shock if you do high depth/breadth on lots of leads.

Slow feedback that kills your investigative flow (you’re in the zone, system feels sluggish).

Mitigations

Make depth/breadth very visible knobs per run:

Have presets: “quick scan”, “standard”, “deep dive”.

Cache by URL:

If source https://example.com/foo is already in evidence, don’t re-summarize it; reuse.

Limit parallelism at first:

Don’t try to run 20 deep dives at once; start with 1–2 concurrent runs.

5. Implementation / maintenance risk
Uncertainty

Open-source deep-research repos:

change APIs

may be less maintained

bring their own dependency stack

Risks

API churn
Upgrading GPT Researcher or DeerFlow might break your adapter.

Dependency hell
Each repo has opinions about Python version, libraries, etc.

Over-commitment to a single framework
If you wire their internals directly into your database, you’re stuck.

Mitigations

Keep the DeepResearchClient abstraction really clean:

Your app knows only:

run_research(query, params) -> ResearchResult

Everything else is adapter-internal.

Pin versions of the chosen library in requirements.txt or poetry/pipenv.

Have a dumb fallback path:

A script that:

builds 3–5 Brave queries

fetches pages

asks the LLM to summarize and list leads
If the fancy engine breaks, you’re not dead.

6. Epistemic risk: how you think with it

You said you’re an epistemological nihilist and care more about:

“what is claimed where”

than “truth” in some absolute sense.

Your design does match that:

Source = where

Evidence = what text we saw

Claim = our structured annotation of that text

But there are still two important human-cognition risks:

Reports feel authoritative
A neatly written report, with citations, feels more grounded than it may be.

Graph structure feels like reality
Seeing a nice graph of entity → relationship → entity makes weak patterns look stronger than they are.

Mitigations are mostly about UX and habits:

Always be able to drop back down to raw snippets quickly.

Consider a simple visual cue in the UI for “confidence”:

of evidence snippets supporting a claim
of distinct domains

Treat the KG as:

map of assertions people have made,
not “the way the world is”.

7. The biggest “unknown unknown”

The main thing we can’t fully nail in advance is:

Which interactions will actually make you go “oh wow, this feels like a real investigative assistant”?

You’ll only discover that by:

wiring up this v1

using it on a real messy topic (like your psychological warfare example)

noticing:

which parts feel magical

which parts feel like friction or fakery

what queries you wish you could express

So if there’s one meta-recommendation:

Treat v1 as an instrumented notebook for your own methodology, not a finished product.

Log everything, keep it legible, and assume you’ll refactor around your actual habits once you’ve lived with it a bit.