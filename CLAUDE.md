# CLAUDE.md - Permanent Section (v2 - Condensed)

---

## **DESIGN PHILOSOPHY**

### **No hardcoded heuristics. Full LLM intelligence. Quality-first.**

**Core Principle**: Trust the LLM to make intelligent decisions based on full context, not programmatic rules.

**Anti-Patterns** (FORBIDDEN):
- ‚ùå Hardcoded thresholds ("drop source after 2 failures")
- ‚ùå Fixed sampling limits (top 5, first 10, etc.)
- ‚ùå Rule-based decision trees ("if X then Y")
- ‚ùå Premature optimization for cost/speed over quality
- ‚ùå **Artificial timeouts on tests** (`timeout 600`, Bash tool timeout parameter on research tasks)

**Correct Approach** (REQUIRED):
- ‚úÖ Give LLM full context and ask for intelligent decisions
- ‚úÖ Make ALL limits user-configurable (not hardcoded)
- ‚úÖ Require LLM to justify all decisions with reasoning
- ‚úÖ Optimize for quality - user configures budget upfront and walks away
- ‚úÖ Use LLM's 1M token context fully (no artificial sampling)
- ‚úÖ **Let tests run naturally** - System has built-in timeouts (LLM 180s, Max research budget). Trust them.

**User Workflow**: Configure parameters once ‚Üí Run research ‚Üí Walk away ‚Üí Get comprehensive results
- No mid-run feedback required
- No manual intervention
- No stopping for user input
- All decisions automated via LLM intelligence

---

## PURPOSE

**CRITICAL**: This file guides EVERY Claude Code API call.

**Structure**:
- **PERMANENT**: Core principles (rarely changes)
- **TEMPORARY**: Current tasks (updated frequently)

**Source Files**: CLAUDE_PERMANENT_v2.md, CLAUDE_TEMP.md, REGENERATE_CLAUDE.md

**When to Update**:
- **TEMPORARY only**: Tasks complete, blockers change, next actions shift
- **PERMANENT**: Major architecture changes, new systematic failure patterns

---

## VISION

**SigInt Platform** - AI-Powered Investigative Journalism
- 15+ government/social/document sources
- Automated Boolean monitoring + email alerts
- AI analysis (summaries, entity extraction, timelines)
- Team collaboration workspaces

**Current**: ~60% complete, 4 databases working
**Timeline**: 3-6 months to production

See: INVESTIGATIVE_PLATFORM_VISION.md (75 pages)

---

## CORE PRINCIPLES (MANDATORY)

### 1. ADVERSARIAL TESTING

**Assume all tests will fail. Prove they didn't.**

- Before claiming success, actively try to break it
- Look for what you CANNOT prove
- Document limitations > fabricate success

‚ùå WRONG: "Integration complete! Everything working!"
‚úÖ RIGHT: "[PASS] Query gen: 5 jobs returned | [UNVERIFIED] Parallel execution, error handling | [LIMITATION] Slow on WSL2 (8s vs 5s)"

### 2. EVIDENCE HIERARCHY

**Only these count**:
1. **Command output** - Highest proof
2. **Error messages** - Proof of failure
3. **User execution** - Ultimate proof

**NOT evidence**:
- "Should work because..." (reasoning ‚â† proof)
- "Code looks correct..." (existence ‚â† proof)
- "Tests passed" without output (claim ‚â† proof)

### 3. FORBIDDEN/REQUIRED LANGUAGE

**NEVER**: "Success!", "Done!", "EXCELLENT!", "Working!", "Perfect!", ‚úÖüéâ‚ú®
**ALWAYS**: "[PASS]", "[FAIL]", "[BLOCKED]", "Test passed:", "Limitation found:", "Unverified:"

### 4. ANTI-LYING CHECKLIST (BEFORE REPORTING)

1. ‚ùì Read COMPLETE output, not just success messages?
2. ‚ùì Count failures AND successes?
3. ‚ùì About to use ‚úÖ emoji? ‚Üí Use [PASS]/[FAIL] instead
4. ‚ùì Ignoring/downplaying failures? ‚Üí Lead with failures
5. ‚ùì Cherry-picking good parts? ‚Üí Report failures first

**Format**: `[FAIL] X: error | [FAIL] Y: error | [PASS] Z: success`

### 5. DISCOVERY BEFORE BUILDING

**BEFORE building, check if it exists**:
1. Read STATUS.md - what's [PASS]?
2. Read CLAUDE.md TEMPORARY - current task?
3. Check directory - does it already exist?
4. Test existing entry points - does it work?

**Why**: Prevents rebuilding existing infrastructure

### 6. CIRCUIT BREAKERS (HARD STOP)

**STOP immediately when**:
1. Import errors on entry points
2. 3+ consecutive timeouts **from the system's built-in timeouts** (not artificial wrappers)
3. Scope drift (doing more than declared)
4. No evidence after 30 minutes
5. Circular work (repeating failed approach)
6. Config file not found
7. API quota/rate limit exceeded

**NEVER impose artificial timeouts**:
- ‚ùå Shell-level timeout wrappers (`timeout 600 python3 script.py`)
- ‚ùå Bash tool timeout parameter on research/test scripts
- ‚úÖ Trust system's built-in protection:
  - LLM call timeout: 180s (3 min) - API failure protection
  - User-configured limits: max_queries_per_source, max_time_per_source_seconds
  - Total research budget: User-configured (e.g., 120 min) - Total cap
- ‚úÖ User configured these upfront - let them work

### 7. FAIL-FAST AND LOUD

- Surface errors with full stack traces
- NEVER swallow exceptions
- Log: API calls, LLM queries, config loads, validation checks

### 8. NO LAZY IMPLEMENTATIONS

**Forbidden**: Mocks in production, fallbacks hiding errors, TODO pseudocode
**Required**: Real implementations or explicit documentation of gaps

### 9. DOCUMENTATION CREATION POLICY

**NEVER create documentation files without user directive or explicit assent**

**Forbidden**:
- ‚ùå Creating .md files "for organization" without asking
- ‚ùå Writing analysis/planning documents unprompted
- ‚ùå Generating README files without request
- ‚ùå Creating summary/review documents on your own initiative

**Required**:
- ‚úÖ Ask user before creating any documentation file
- ‚úÖ Only create docs when explicitly requested
- ‚úÖ If user asks "organize X", ask WHERE and WHAT FORMAT before creating files

**Exception**: Files that ARE part of implementation (code comments, docstrings, inline docs) are always allowed.

**Project-Specific Locations**:
- Active docs: `/home/brian/sam_gov/docs/` (implementation guides, technical references)
- Archive: `/home/brian/sam_gov/archive/YYYY-MM-DD/` (completed work)

---

## DIRECTORY STRUCTURE

```
sam_gov/
‚îú‚îÄ‚îÄ CLAUDE.md, STATUS.md, PATTERNS.md
‚îú‚îÄ‚îÄ prompts/                 # Jinja2 LLM prompt templates
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # Template environment
‚îÇ   ‚îú‚îÄ‚îÄ deep_research/       # Deep research prompts (.j2)
‚îÇ   ‚îî‚îÄ‚îÄ integrations/        # Integration prompts (.j2)
‚îú‚îÄ‚îÄ core/                    # Research engines + prompt_loader.py
‚îú‚îÄ‚îÄ integrations/            # Data source adapters
‚îÇ   ‚îú‚îÄ‚îÄ government/          # SAM, DVIDS, USAJobs, ClearanceJobs
‚îÇ   ‚îî‚îÄ‚îÄ social/              # Twitter, Reddit, Discord
‚îú‚îÄ‚îÄ apps/                    # User entry points (ai_research.py, unified_search_app.py)
‚îú‚îÄ‚îÄ tests/                   # All test scripts
‚îú‚îÄ‚îÄ research/                # Deep research workflows
‚îú‚îÄ‚îÄ data/                    # Runtime storage (articles, exports, logs, monitors)
‚îú‚îÄ‚îÄ docs/                    # Documentation
‚îú‚îÄ‚îÄ archive/YYYY-MM-DD/      # Archived code/docs
‚îú‚îÄ‚îÄ .env, config.yaml        # Configuration (gitignored)
‚îî‚îÄ‚îÄ llm_utils.py, config_loader.py
```

### Root Directory Discipline

**Only in Root** (~15 files):
- Core docs (9): CLAUDE.md, STATUS.md, PATTERNS.md, etc.
- Config (4-5): .env, .gitignore, requirements.txt, config*.yaml
- Core utils (2): llm_utils.py, config_loader.py

**Archive immediately**:
- Test scripts ‚Üí `tests/`
- *_COMPLETE.md, *_STATUS.md ‚Üí `archive/YYYY-MM-DD/docs/`
- *_PLAN.md ‚Üí `docs/` (active) or `archive/` (completed)
- *.log, temp_*.txt ‚Üí `data/logs/`

**If root >20 files**: STOP, archive excess, create README

### File Finding

- Current task ‚Üí CLAUDE.md TEMPORARY
- Is X working? ‚Üí STATUS.md
- How to implement Y? ‚Üí PATTERNS.md
- Why? ‚Üí INVESTIGATIVE_PLATFORM_VISION.md
- Archived? ‚Üí archive/YYYY-MM-DD/README.md

### Documentation Layers

1. **VISION** (INVESTIGATIVE_PLATFORM_VISION.md) - What we want
2. **ROADMAP** (ROADMAP.md) - How we plan it
3. **STATUS** (STATUS.md) - What actually works
4. **CLAUDE.md TEMPORARY** - What I'm doing now

**Before work**: Read TEMPORARY ‚Üí STATUS ‚Üí Test existing ‚Üí Only build if missing/broken

---

## SYSTEM ARCHITECTURE

### Multi-Agent Research System

**Pattern**: Manager-Agent with Hypothesis Branching (evolved BabyAGI-like)
**Complexity**: 4,392 lines (research/deep_research.py)
**Philosophy**: No hardcoded heuristics, full LLM intelligence, quality-first

### Agent Hierarchy

```
User Query ‚Üí [Task Decomposition LLM] ‚Üí 3-5 research tasks
    ‚Üì
[Manager LLM] ‚Üí Prioritizes tasks (P1-P10)
    ‚Üì
For each task:
    ‚îú‚îÄ [Hypothesis Generation LLM] ‚Üí 3-5 investigative hypotheses
    ‚îú‚îÄ For each hypothesis:
    ‚îÇ   ‚îú‚îÄ [Query Generation LLM] ‚Üí Source-specific queries
    ‚îÇ   ‚îú‚îÄ [Source Execution] ‚Üí DVIDS, Brave, SAM.gov, etc.
    ‚îÇ   ‚îú‚îÄ [Relevance Filter LLM] ‚Üí Filter hypothesis results
    ‚îÇ   ‚îî‚îÄ [Coverage Assessment LLM] ‚Üí Should we continue?
    ‚îî‚îÄ [Relevance Filter LLM] ‚Üí Filter main task results (if no hypotheses)
        ‚Üì
[Report Synthesis LLM] ‚Üí Final markdown report
```

### Key Components

**research/deep_research.py** (4,392 lines):
- Main class: `SimpleDeepResearch` (misnomer - not simple!)
- Task decomposition, hypothesis generation, filtering, synthesis
- 5 phases implemented (see STATUS.md for details)

**research/execution_logger.py**:
- Structured logging to `execution_log.jsonl`
- All LLM decisions, API calls, filtering choices logged

**prompts/deep_research/** (Jinja2 templates):
- 8+ prompts for different LLM agents
- All prompts version-controlled separately from code

**8 integrations** (via MCP + direct):
- Government: SAM.gov, DVIDS, USAJobs, ClearanceJobs
- Social: Twitter, Reddit, Discord
- Web: Brave Search

### Output Structure

```
data/research_output/YYYY-MM-DD_HH-MM-SS_query/
‚îú‚îÄ execution_log.jsonl  # Structured event log (all LLM decisions)
‚îú‚îÄ metadata.json        # Run metadata (config, stats, coverage decisions)
‚îú‚îÄ report.md           # Final markdown report
‚îú‚îÄ results.json        # All results with deduplication
‚îî‚îÄ raw/                # Raw API responses
```

### Entry Points

**Primary**:
- `apps/ai_research.py` - Main CLI for deep research
- `apps/unified_search_app.py` - Streamlit web UI

**Testing**:
- `tests/test_deep_research_full.py` - E2E validation
- 160+ test files for all components

---

## ENVIRONMENT

### Python (.venv)

**MANDATORY**: ALL commands must activate `.venv` first

```bash
source .venv/bin/activate  # FIRST, EVERY TIME
python3 script.py          # Now uses .venv
```

**Circuit Breaker**: `ModuleNotFoundError: No module named 'playwright'`
‚Üí STOP, activate `.venv`, rerun

### API Keys

**ALWAYS use python-dotenv**:

```python
from dotenv import load_dotenv
import os

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')
```

**NEVER**: Hardcode keys, commit .env, use os.environ without load_dotenv()

---

## CODE PATTERNS

### LLM Calls

```python
from llm_utils import acompletion
from dotenv import load_dotenv

load_dotenv()

response = await acompletion(
    model="gpt-5-mini",
    messages=[{"role": "user", "content": prompt}],
    response_format={"type": "json_schema", "json_schema": {...}}
)
```

**NEVER**: Call litellm directly, use max_tokens/max_output_tokens (breaks gpt-5)

### LLM Prompts with JSON Examples

**ALWAYS use Jinja2 templates** for prompts containing JSON examples.

**Pattern**:
```python
from core.prompt_loader import render_prompt

prompt = render_prompt(
    "deep_research/query_reformulation.j2",
    research_question=question,
    original_query=query,
    results_count=count
)
```

**Template Location**: `prompts/<module>/<prompt_name>.j2`

**Key Benefits**:
- NO `{{` `}}` escaping needed (JSON examples are literal)
- Prompts version-controlled separately from code
- Easy to edit without touching Python
- Templates are testable and parseable

**Reference**:
- docs/FSTRING_JSON_METHODOLOGY.md (comprehensive guide)
- docs/JINJA2_MIGRATION_INVESTIGATION.md (migration details)

**NEVER**:
- Use f-strings with `{{` `}}` escaping for JSON examples
- Mix JSON structure literals with Python f-string variables

### Database Integration

**Copy from**: `integrations/government/sam_integration.py`

**Required**:
```python
class NewIntegration(DatabaseIntegration):
    @property
    def metadata(self) -> DatabaseMetadata: ...
    async def is_relevant(self, question: str) -> bool: ...
    async def generate_query(self, question: str) -> Optional[Dict]: ...
    async def execute_search(self, params, key, limit) -> QueryResult: ...
```

**After creating**:
1. Save in `integrations/government/` or `social/`
2. Register in `integrations/registry.py`
3. Create `tests/test_newsource_live.py`
4. Test E2E: `python3 apps/ai_research.py "query"`
5. Update STATUS.md with [PASS]/[BLOCKED]

---

## TESTING

### Categories

1. **Unit**: Isolated, may mock, fast
2. **Integration**: Multiple components, real APIs
3. **E2E** (REQUIRED for success claims): User-facing entry points, no mocks

### Checklist (Before Claiming Success)

ALL must be true:
- [ ] Imports resolve
- [ ] Config loads from .env/config.yaml
- [ ] API calls succeed
- [ ] Results match format
- [ ] Errors handled gracefully
- [ ] User can execute without assistance
- [ ] No timeouts
- [ ] Logging works
- [ ] Cost tracking (if LLM)

**If ANY false ‚Üí NOT succeeded**

---

## KNOWN ISSUES

1. **gpt-5 models**: NEVER use max_tokens (breaks reasoning). Use llm_utils.acompletion()
2. **ClearanceJobs**: Official API broken. Use clearancejobs_playwright.py (slower but accurate)
3. **USAJobs**: Requires headers: `User-Agent: email`, `Authorization-Key: [key]`

---

## ESSENTIAL TOOLS

**Keep** (complex/critical):
- prompts/ directory + all .j2 templates
- core/prompt_loader.py (Jinja2 infrastructure)
- tests/test_all_four_databases.py
- tests/test_verification.py
- llm_utils.py, config_loader.py
- integrations/government/sam_integration.py

**Regenerate on-demand** (simple):
- Unit tests, CLI scripts, utilities

**Entry Points** (always keep):
- apps/ai_research.py
- apps/unified_search_app.py

---

## QUICK REFERENCE

```bash
source .venv/bin/activate  # ALWAYS FIRST

# Test entry points
python3 apps/ai_research.py "query"
python3 tests/test_verification.py
streamlit run apps/unified_search_app.py

# Verify environment
which python3  # Should show .venv/bin/python3
pip list | grep playwright
```

---

**END OF PERMANENT SECTION**
# CLAUDE.md - Temporary Section (Condensed)

**Last Updated**: 2025-11-22
**Current Branch**: `master`
**Current Phase**: Research Quality Improvements
**Status**: Implementing source selection bug fixes and enhanced logging

---

## CURRENT STATUS

**Recent Fixes** (2025-11-22):
- ‚úÖ Follow-up task redundancy fixed (global task context added)
- ‚úÖ Docker infrastructure complete (Chrome + Playwright pre-installed)

**Active Investigation** (2025-11-22):
- üîç Reddit source selection bug (selected but barely queried: 3 vs 59 for Discord)
- üîç Source skipping logic (why are selected sources not executed?)
- üîç Logging gaps (need visibility into all LLM decisions)

---

## NEXT PLANNED WORK

### HIGH PRIORITY (In Progress)

**1. Query Quality Assessment (Saturation Enhancement)** üîÑ **IN PROGRESS**
- **Problem**: Reddit/other sources generating overly specific queries ‚Üí 0 results
- **Root Cause**: Queries too restrictive (multiple AND operators, overly specific)
- **Solution**: Enhance saturation prompt with general query quality assessment
- **Approach** (LLM intelligence, not hardcoded heuristics):
  - Saturation LLM analyzes query_history: results_total, results_accepted, effectiveness
  - Uses prose reasoning to identify issues (0 results, low acceptance, wrong domain, etc.)
  - Suggests improvements based on source_metadata (what this source supports)
  - No predefined quality categories - LLM reasons intelligently
  - Single LLM call (no extra analysis call) - efficient
- **Architecture Principles**:
  - ‚úÖ Give LLM goal + context, let it reason (not hardcoded categories)
  - ‚úÖ Leverage existing query_history metrics (results_total, effectiveness)
  - ‚úÖ Use source_metadata for source-specific capabilities
  - ‚úÖ Prose reasoning (not structured "quality_issues" categories)
- **Implementation**:
  - Enhance prompts/deep_research/source_saturation.j2 with query quality guidance
  - Remove separate zero_result_analysis.j2 (too narrow)
  - Keep simple schema - reasoning field explains quality assessment
- **Files**: prompts/deep_research/source_saturation.j2, research/deep_research.py
- **Status**: ‚úÖ COMPLETE (2025-11-22) - Saturation prompt enhanced, zero-result analysis removed

**2. Hypothesis Diversity Enhancement** ‚úÖ **COMPLETE**
- **Problem**: Multiple hypotheses within same task may investigate overlapping angles
- **Root Cause**: Hypothesis generation LLM doesn't see existing tasks or other hypotheses
- **Solution Implemented** (2025-11-22):
  - ‚úÖ Updated `_generate_hypotheses()` method signature to accept `all_tasks` and `existing_hypotheses`
  - ‚úÖ Modified both call sites (lines 653, 870) to pass context
  - ‚úÖ Enhanced hypothesis_generation.j2 template with "CONTEXT - AVOID DUPLICATION" section
  - ‚úÖ Added diversity guidance referencing existing tasks and hypotheses
  - ‚úÖ Verified imports and prompt rendering work correctly
- **Architecture**:
  - ‚úÖ Extends existing pattern from follow-up generation (clean, no code duplication)
  - ‚úÖ No hardcoded similarity checks or overlap thresholds
  - ‚úÖ Declarative - LLM uses context to ensure diversity via reasoning
  - ‚úÖ Scales to new sources - no source-specific logic
- **Files Modified**:
  - research/deep_research.py (method signature, 2 call sites, context formatting)
  - prompts/deep_research/hypothesis_generation.j2 (context section, enhanced diversity guidance)
- **Status**: ‚úÖ COMPLETE (2025-11-22) - Implementation complete, imports verified

**3. Enhanced Structured Logging** üìã **APPROVED - NOT STARTED**
- **Goal**: Detailed visibility into ALL decisions and time usage
- **New event types needed**:
  ```python
  {
    "event_type": "source_skipped",
    "source": "Reddit",
    "reason": "is_relevant returned False" | "generate_query failed" | "timeout",
    "task_id": 0,
    "hypothesis_id": 1
  }

  {
    "event_type": "zero_results_analysis",
    "source": "SAM.gov",
    "query": "...",
    "llm_assessment": "Query too specific - no violations exist",
    "should_reformulate": false
  }

  {
    "event_type": "time_breakdown",
    "task_id": 0,
    "hypothesis_id": 1,
    "source": "Brave",
    "time_query_generation_ms": 234,
    "time_api_call_ms": 1523,
    "time_filtering_ms": 456,
    "total_time_ms": 2213
  }
  ```
- **Files**: research/execution_logger.py, research/deep_research.py

### MEDIUM PRIORITY

**4. Discord Parser Robustness** üìã **CHECK IF FIXED**
- **Status**: User reports "the problem that caused that has been fixed"
- **Action**: Verify fix is in place for malformed JSON lines (0.14% error rate)
- **Action**: Add try/catch if not already present
- **Files**: integrations/social/discord_integration.py

**5. Time Budget Configuration** üìã **APPROVED**
- **Goal**: Set very high defaults + detailed time logging
- **Actions**:
  - Increase config.yaml defaults: `max_time_minutes: 240` (4 hours)
  - Add time breakdown logging (see #3 above)
  - Skip dynamic budgeting (user confirmed: overengineered)
- **Files**: config.yaml, research/deep_research.py

### LOW PRIORITY

**6. Source Context Documentation** üìã **NOT STARTED**
- **Goal**: Better explain what each source provides in prompts
- **Add to hypothesis_generation.j2**:
  - "Reddit provides: r/defense, r/govcontracts, r/Intelligence, r/geopolitics discussions"
  - "Discord provides: Bellingcat OSINT server, Project OWL geopolitics, OSINT community"
- **Files**: prompts/deep_research/hypothesis_generation.j2

---

## WHAT'S WORKING

**Core Research Engine**: `apps/ai_research.py`
- Task decomposition with angle-based queries (not entity permutations)
- Hypothesis generation (3-5 per task)
- **NEW**: Follow-up generation with global task context (prevents redundancy)
- Sequential hypothesis execution with coverage assessment
- LLM-powered follow-up generation (addresses info gaps)
- Manager LLM prioritizes pending tasks (P1-P10)
- Saturation detection (stops when research complete)

**Integrations**: 8 working
- Government: SAM.gov, DVIDS, USAJobs, ClearanceJobs
- Social: Twitter, Reddit, Discord
- Web: Brave Search

**Key Features**:
- Jinja2 prompt templates (no f-string JSON escaping)
- Cost tracking with fallback model chain
- Cross-attempt result accumulation
- Deduplication (60-80% typical)
- Entity extraction and relationship discovery
- **NEW**: Follow-up tasks see all existing tasks to avoid duplication

---

## KNOWN ISSUES

1. **gpt-5 models**: Never use max_tokens (breaks reasoning). Use llm_utils.acompletion()
2. **ClearanceJobs**: Official API broken. Use clearancejobs_playwright.py
3. **USAJobs**: Requires headers: `User-Agent: email`, `Authorization-Key: [key]`
4. **Discord**: 14/9916 exports malformed (0.14%) - gracefully skipped with warnings
5. **SAM.gov**: Low rate limits - will be rate-limited early in research (handled gracefully)

---

## RECENT CHANGES (Last 7 Days)

**2025-11-22**: Reddit underutilization fix (commit 503b13d)
- ‚úÖ Replaced hardcoded keyword filtering with LLM-based relevance check
- ‚úÖ Reddit now returns True for contract queries (previously returned False)
- ‚úÖ Consistent with Discord integration pattern
- Files modified: integrations/social/reddit_integration.py (60 lines), tests/test_reddit_fix.py (new)
- Impact: Reddit will now be queried proportionally to its relevance, capturing community discussions and insider perspectives

**2025-11-22**: Follow-up task redundancy fix
- ‚úÖ Added global task context to follow-up generation
- ‚úÖ Follow-up LLM now sees all completed + pending tasks
- ‚úÖ Prevents creating duplicate follow-ups
- Files modified: research/deep_research.py (17 lines), prompts/deep_research/follow_up_generation.j2 (15 lines + documentation)
- Impact: Should eliminate near-duplicate follow-ups like Anduril Tasks 4-7

**2025-11-22**: Docker infrastructure created
- ‚úÖ Dockerfile with Chrome + Playwright pre-installed
- ‚úÖ docker-compose.yml for research CLI and web UI
- ‚úÖ .dockerignore for optimized builds
- ‚úÖ DOCKER.md documentation
- Impact: Solves WSL2 Chrome binary path issues permanently

**2025-11-21**: Phase 1 query saturation COMPLETE
- ‚úÖ Multi-query saturation with LLM intelligence
- ‚úÖ Three-tier exit strategy
- ‚úÖ 54% more results than baseline (SpaceX test: 80 vs 52)
- Files modified: ~482 lines across research/deep_research.py, integrations/source_metadata.py, etc.

---

**END OF TEMPORARY SECTION**
